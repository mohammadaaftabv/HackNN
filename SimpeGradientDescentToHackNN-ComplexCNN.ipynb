{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1af6b87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#imports necessary to define a neural network \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#ensure you are using GPU.\n",
    "if torch.cuda.is_available():  \n",
    "  dev = \"cuda:0\" \n",
    "else:  \n",
    "  dev = \"cpu\"  \n",
    "\n",
    "device = torch.device(dev)\n",
    "print(device)\n",
    "\n",
    "dtype = torch.cuda.FloatTensor\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59434201",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                               transforms.Normalize(-0.5,0.5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0d64c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohammad/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1631630815121/work/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "trainset = datasets.MNIST('PATH_TO_STORE_TRAINSET', download=True, train=True, transform=transform)\n",
    "valset = datasets.MNIST('PATH_TO_STORE_TESTSET', download=True, train=False, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb82b7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "855be1f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 60 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure = plt.figure()\n",
    "num_of_images = 60\n",
    "for index in range(1, num_of_images + 1):\n",
    "    plt.subplot(6, 10, index)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(images[index].numpy().squeeze(), cmap='gray_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b808f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "model = Net()\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5d80b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-121c0ab709c4>:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "images, labels = next(iter(trainloader))\n",
    "logps = model(images.reshape([64,1,28,28]).cuda()) #log probabilities\n",
    "loss = criterion(logps, labels.cuda()) #calculate the NLL loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef5b40c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-121c0ab709c4>:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 1.054487051581269\n",
      "\n",
      "Training Time (in minutes) = 0.09986407359441121\n",
      "Epoch 1 - Training loss: 0.4296170888361392\n",
      "\n",
      "Training Time (in minutes) = 0.19950389862060547\n",
      "Epoch 2 - Training loss: 0.34607321034267\n",
      "\n",
      "Training Time (in minutes) = 0.30101222594579063\n",
      "Epoch 3 - Training loss: 0.30589389079002177\n",
      "\n",
      "Training Time (in minutes) = 0.40597699880599974\n",
      "Epoch 4 - Training loss: 0.2773636725824525\n",
      "\n",
      "Training Time (in minutes) = 0.509467613697052\n",
      "Epoch 5 - Training loss: 0.258386384640167\n",
      "\n",
      "Training Time (in minutes) = 0.6131433447202047\n",
      "Epoch 6 - Training loss: 0.23890492706490096\n",
      "\n",
      "Training Time (in minutes) = 0.7178377707799276\n",
      "Epoch 7 - Training loss: 0.22986116852047347\n",
      "\n",
      "Training Time (in minutes) = 0.8197766065597534\n",
      "Epoch 8 - Training loss: 0.21903355783626025\n",
      "\n",
      "Training Time (in minutes) = 0.9185047666231791\n",
      "Epoch 9 - Training loss: 0.20840795196767556\n",
      "\n",
      "Training Time (in minutes) = 1.0175398111343383\n",
      "Epoch 10 - Training loss: 0.2015412354778284\n",
      "\n",
      "Training Time (in minutes) = 1.116007653872172\n",
      "Epoch 11 - Training loss: 0.19523519819090043\n",
      "\n",
      "Training Time (in minutes) = 1.215511389573415\n",
      "Epoch 12 - Training loss: 0.1854197568178717\n",
      "\n",
      "Training Time (in minutes) = 1.3143096486727397\n",
      "Epoch 13 - Training loss: 0.18071069512297827\n",
      "\n",
      "Training Time (in minutes) = 1.414023800690969\n",
      "Epoch 14 - Training loss: 0.17544507534145865\n",
      "\n",
      "Training Time (in minutes) = 1.5129013299942016\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.003, momentum=0.9)\n",
    "time0 = time()\n",
    "epochs = 15\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        \n",
    "        # Training pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(images.reshape([-1,1,28,28]).cuda())\n",
    "        loss = criterion(output, labels.cuda())\n",
    "        \n",
    "        #This is where the model learns by backpropagating\n",
    "        loss.backward()\n",
    "        \n",
    "        #And optimizes its weights here\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(trainloader)))\n",
    "        print(\"\\nTraining Time (in minutes) =\",(time()-time0)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfc34b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Digit = 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-121c0ab709c4>:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def view_classify(img, ps):\n",
    "    ''' Function for viewing an image and it's predicted classes.\n",
    "    '''\n",
    "    ps = ps.cpu().data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(np.arange(10), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    ax2.set_yticklabels(np.arange(10))\n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1.1)\n",
    "    plt.tight_layout()\n",
    "images, labels = next(iter(valloader))\n",
    "\n",
    "img = images[0].reshape([-1,1,28,28])\n",
    "with torch.no_grad():\n",
    "    logps = model(img.cuda())\n",
    "\n",
    "ps = torch.exp(logps)\n",
    "probab = list(ps.detach().cpu().numpy()[0])\n",
    "print(\"Predicted Digit =\", probab.index(max(probab)))\n",
    "view_classify(img.view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a237d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-121c0ab709c4>:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[3.5439e-16, 7.0265e-19, 1.2701e-11, 1.0988e-10, 2.4161e-14, 8.6821e-11,\n",
       "         3.1597e-23, 1.0000e+00, 6.5452e-14, 2.7657e-09]], device='cuda:0',\n",
       "       grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(model(img.cuda()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "759516a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-121c0ab709c4>:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of Images Tested = 10000\n",
      "\n",
      "Model Accuracy = 0.954\n"
     ]
    }
   ],
   "source": [
    "correct_count, all_count = 0, 0\n",
    "for images,labels in valloader:\n",
    "  for i in range(len(labels)):\n",
    "    img = images[i].reshape([-1,1,28,28])\n",
    "    with torch.no_grad():\n",
    "        logps = model(img.cuda())\n",
    "\n",
    "    \n",
    "    ps = torch.exp(logps)\n",
    "    probab = list(ps.detach().cpu().numpy()[0])\n",
    "    pred_label = probab.index(max(probab))\n",
    "    true_label = labels.numpy()[i]\n",
    "    if(true_label == pred_label):\n",
    "      correct_count += 1\n",
    "    all_count += 1\n",
    "\n",
    "print(\"Number Of Images Tested =\", all_count)\n",
    "print(\"\\nModel Accuracy =\", (correct_count/all_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c79a61cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 28, 28])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14336c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6aaa3f9700>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img.detach().cpu().numpy().reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "97e4cbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findloss(diff,pred):\n",
    "    \n",
    "#     diff is array of noise\n",
    "#     pred is the prediction of the above trained model\n",
    "    \n",
    "    \n",
    "    l1=(torch.square(diff))\n",
    "    l1=torch.mean(l1)\n",
    "    \n",
    "    l2=pred**(-2)/1e4\n",
    "    \n",
    "    fl=l1+l2\n",
    "    fl=fl**0.5\n",
    "    \n",
    "    return fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "47a7cb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff=torch.rand([784]).cuda().requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d9612f57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-121c0ab709c4>:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss and prediction by the model after 0 steps of gradient descent are 0.6074830293655396 0.06414470076560974\n",
      "Loss and prediction by the model after 10000 steps of gradient descent are 0.7411953806877136 0.39648962020874023\n",
      "Loss and prediction by the model after 20000 steps of gradient descent are 0.7403720021247864 0.9749667048454285\n",
      "Loss and prediction by the model after 30000 steps of gradient descent are 0.7397850751876831 0.9426537752151489\n",
      "Loss and prediction by the model after 40000 steps of gradient descent are 0.7400489449501038 0.2680071294307709\n",
      "Loss and prediction by the model after 50000 steps of gradient descent are 0.738592267036438 0.9722388982772827\n",
      "Loss and prediction by the model after 60000 steps of gradient descent are 0.738010823726654 0.9761130213737488\n",
      "Loss and prediction by the model after 70000 steps of gradient descent are 0.7374500036239624 0.9017658233642578\n",
      "Loss and prediction by the model after 80000 steps of gradient descent are 0.7368385791778564 0.9912686347961426\n",
      "Loss and prediction by the model after 90000 steps of gradient descent are 0.738295316696167 0.17687752842903137\n",
      "Loss and prediction by the model after 100000 steps of gradient descent are 0.7355937361717224 0.8780509233474731\n",
      "Loss and prediction by the model after 110000 steps of gradient descent are 0.7349809408187866 0.9955593347549438\n",
      "Loss and prediction by the model after 120000 steps of gradient descent are 0.7345524430274963 0.5078142881393433\n",
      "Loss and prediction by the model after 130000 steps of gradient descent are 0.7337512373924255 0.9435663223266602\n",
      "Loss and prediction by the model after 140000 steps of gradient descent are 0.7330947518348694 0.9766626954078674\n",
      "Loss and prediction by the model after 150000 steps of gradient descent are 0.7324784994125366 0.8655863404273987\n",
      "Loss and prediction by the model after 160000 steps of gradient descent are 0.7318615913391113 0.9944041967391968\n",
      "Loss and prediction by the model after 170000 steps of gradient descent are 0.7312355041503906 0.999416172504425\n",
      "Loss and prediction by the model after 180000 steps of gradient descent are 0.7305245399475098 0.9988154172897339\n",
      "Loss and prediction by the model after 190000 steps of gradient descent are 0.7304438352584839 0.3430173099040985\n",
      "Loss and prediction by the model after 200000 steps of gradient descent are 0.72934889793396 0.9682080745697021\n",
      "Loss and prediction by the model after 210000 steps of gradient descent are 0.7287586331367493 0.9622701406478882\n",
      "Loss and prediction by the model after 220000 steps of gradient descent are 0.7281598448753357 0.9997528195381165\n",
      "Loss and prediction by the model after 230000 steps of gradient descent are 0.7275837659835815 0.9960416555404663\n",
      "Loss and prediction by the model after 240000 steps of gradient descent are 0.7269991636276245 0.9951962828636169\n",
      "Loss and prediction by the model after 250000 steps of gradient descent are 0.7264052033424377 0.9984999299049377\n",
      "Loss and prediction by the model after 260000 steps of gradient descent are 0.7258123159408569 0.9997755289077759\n",
      "Loss and prediction by the model after 270000 steps of gradient descent are 0.7254382967948914 0.489749014377594\n",
      "Loss and prediction by the model after 280000 steps of gradient descent are 0.7246819138526917 0.7377921938896179\n",
      "Loss and prediction by the model after 290000 steps of gradient descent are 0.7240346670150757 0.999959409236908\n",
      "Loss and prediction by the model after 300000 steps of gradient descent are 0.7237036228179932 0.45227497816085815\n",
      "Loss and prediction by the model after 310000 steps of gradient descent are 0.7228261828422546 0.9996538162231445\n",
      "Loss and prediction by the model after 320000 steps of gradient descent are 0.7222625613212585 0.9762007594108582\n",
      "Loss and prediction by the model after 330000 steps of gradient descent are 0.7216871976852417 0.8912135362625122\n",
      "Loss and prediction by the model after 340000 steps of gradient descent are 0.727490246295929 0.10320395231246948\n",
      "Loss and prediction by the model after 350000 steps of gradient descent are 0.7204917669296265 0.9998382329940796\n",
      "Loss and prediction by the model after 360000 steps of gradient descent are 0.7199030518531799 0.9572705030441284\n",
      "Loss and prediction by the model after 370000 steps of gradient descent are 0.719359278678894 0.7602660655975342\n",
      "Loss and prediction by the model after 380000 steps of gradient descent are 0.718718409538269 0.9995435476303101\n",
      "Loss and prediction by the model after 390000 steps of gradient descent are 0.7181314826011658 0.9920927882194519\n",
      "Loss and prediction by the model after 400000 steps of gradient descent are 0.7175543904304504 0.964156985282898\n",
      "Loss and prediction by the model after 410000 steps of gradient descent are 0.7169795036315918 0.9100961089134216\n",
      "Loss and prediction by the model after 420000 steps of gradient descent are 0.7163825035095215 0.9999421834945679\n",
      "Loss and prediction by the model after 430000 steps of gradient descent are 0.716342568397522 0.33757638931274414\n",
      "Loss and prediction by the model after 440000 steps of gradient descent are 0.715227484703064 0.923059344291687\n",
      "Loss and prediction by the model after 450000 steps of gradient descent are 0.7146307826042175 0.9847099781036377\n",
      "Loss and prediction by the model after 460000 steps of gradient descent are 0.7658237814903259 0.03610895946621895\n",
      "Loss and prediction by the model after 470000 steps of gradient descent are 0.7139811515808105 0.35246866941452026\n",
      "Loss and prediction by the model after 480000 steps of gradient descent are 0.7139312028884888 0.25292307138442993\n",
      "Loss and prediction by the model after 490000 steps of gradient descent are 0.7123353481292725 0.9990200996398926\n",
      "Loss and prediction by the model after 500000 steps of gradient descent are 0.7117640376091003 0.9696767926216125\n",
      "Loss and prediction by the model after 510000 steps of gradient descent are 0.7111496925354004 0.9999325275421143\n",
      "Loss and prediction by the model after 520000 steps of gradient descent are 0.7105693817138672 0.997900664806366\n",
      "Loss and prediction by the model after 530000 steps of gradient descent are 0.709993302822113 0.9980885982513428\n",
      "Loss and prediction by the model after 540000 steps of gradient descent are 0.7098970413208008 0.35855504870414734\n",
      "Loss and prediction by the model after 550000 steps of gradient descent are 0.7088407874107361 0.9999791979789734\n",
      "Loss and prediction by the model after 560000 steps of gradient descent are 0.7082647681236267 0.9998961091041565\n",
      "Loss and prediction by the model after 570000 steps of gradient descent are 0.70768141746521 0.9995498061180115\n",
      "Loss and prediction by the model after 580000 steps of gradient descent are 0.707103431224823 0.9956044554710388\n",
      "Loss and prediction by the model after 590000 steps of gradient descent are 0.7065651416778564 0.8208787441253662\n",
      "Loss and prediction by the model after 600000 steps of gradient descent are 0.7059633135795593 0.9094923734664917\n",
      "Loss and prediction by the model after 610000 steps of gradient descent are 0.7055220603942871 0.5600489377975464\n",
      "Loss and prediction by the model after 620000 steps of gradient descent are 0.7047767639160156 0.9998668432235718\n",
      "Loss and prediction by the model after 630000 steps of gradient descent are 0.7708129286766052 0.03188817948102951\n",
      "Loss and prediction by the model after 640000 steps of gradient descent are 0.7036463618278503 0.8975508809089661\n",
      "Loss and prediction by the model after 650000 steps of gradient descent are 0.7030595541000366 0.968445360660553\n",
      "Loss and prediction by the model after 660000 steps of gradient descent are 0.7024797797203064 0.9994786977767944\n",
      "Loss and prediction by the model after 670000 steps of gradient descent are 0.7020653486251831 0.5500081181526184\n",
      "Loss and prediction by the model after 680000 steps of gradient descent are 0.7013211250305176 0.9999942183494568\n",
      "Loss and prediction by the model after 690000 steps of gradient descent are 0.7007421255111694 0.9999907612800598\n",
      "Loss and prediction by the model after 700000 steps of gradient descent are 0.7001628279685974 0.9998918771743774\n",
      "Loss and prediction by the model after 710000 steps of gradient descent are 0.6995927095413208 0.9977351427078247\n",
      "Loss and prediction by the model after 720000 steps of gradient descent are 0.6990198493003845 0.9737087488174438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss and prediction by the model after 730000 steps of gradient descent are 0.6984655857086182 0.876376211643219\n",
      "Loss and prediction by the model after 740000 steps of gradient descent are 0.697870135307312 0.9644328355789185\n",
      "Loss and prediction by the model after 750000 steps of gradient descent are 0.6972860097885132 0.9972715973854065\n",
      "Loss and prediction by the model after 760000 steps of gradient descent are 0.6967243552207947 0.8948680758476257\n",
      "Loss and prediction by the model after 770000 steps of gradient descent are 0.6961249709129333 0.998038113117218\n",
      "Loss and prediction by the model after 780000 steps of gradient descent are 0.6955479383468628 0.9998045563697815\n",
      "Loss and prediction by the model after 790000 steps of gradient descent are 0.6973645687103271 0.1707283854484558\n",
      "Loss and prediction by the model after 800000 steps of gradient descent are 0.6943972706794739 0.9922091960906982\n",
      "Loss and prediction by the model after 810000 steps of gradient descent are 0.6938392519950867 0.8935045003890991\n",
      "Loss and prediction by the model after 820000 steps of gradient descent are 0.6932525634765625 0.9292868971824646\n",
      "Loss and prediction by the model after 830000 steps of gradient descent are 0.6926526427268982 0.9989319443702698\n",
      "Loss and prediction by the model after 840000 steps of gradient descent are 0.6920821666717529 0.9762576222419739\n",
      "Loss and prediction by the model after 850000 steps of gradient descent are 0.6915078163146973 0.9999732375144958\n",
      "Loss and prediction by the model after 860000 steps of gradient descent are 0.6909307241439819 0.9934073090553284\n",
      "Loss and prediction by the model after 870000 steps of gradient descent are 0.6903538107872009 0.9999929070472717\n",
      "Loss and prediction by the model after 880000 steps of gradient descent are 0.689771831035614 0.9985900521278381\n",
      "Loss and prediction by the model after 890000 steps of gradient descent are 0.6891986131668091 0.9957041144371033\n",
      "Loss and prediction by the model after 900000 steps of gradient descent are 0.6891035437583923 0.36180469393730164\n",
      "Loss and prediction by the model after 910000 steps of gradient descent are 0.6880481839179993 0.9995832443237305\n",
      "Loss and prediction by the model after 920000 steps of gradient descent are 0.6874600052833557 0.9892746210098267\n",
      "Loss and prediction by the model after 930000 steps of gradient descent are 0.6868823766708374 0.9972655177116394\n",
      "Loss and prediction by the model after 940000 steps of gradient descent are 0.686318576335907 0.9360896944999695\n",
      "Loss and prediction by the model after 950000 steps of gradient descent are 0.6857479214668274 0.9620789289474487\n",
      "Loss and prediction by the model after 960000 steps of gradient descent are 0.6852333545684814 0.7268252372741699\n",
      "Loss and prediction by the model after 970000 steps of gradient descent are 0.6845921277999878 0.9978720545768738\n",
      "Loss and prediction by the model after 980000 steps of gradient descent are 0.6840200424194336 0.9955043792724609\n",
      "Loss and prediction by the model after 990000 steps of gradient descent are 0.6834456920623779 0.9963958859443665\n",
      "Loss and prediction by the model after 1000000 steps of gradient descent are 0.6828699111938477 0.9977665543556213\n",
      "Loss and prediction by the model after 1010000 steps of gradient descent are 0.6822963356971741 0.9522938132286072\n",
      "Loss and prediction by the model after 1020000 steps of gradient descent are 0.6833112835884094 0.2092553675174713\n",
      "Loss and prediction by the model after 1030000 steps of gradient descent are 0.6811479330062866 0.940634548664093\n",
      "Loss and prediction by the model after 1040000 steps of gradient descent are 0.6805618405342102 0.993561327457428\n",
      "Loss and prediction by the model after 1050000 steps of gradient descent are 0.6800906658172607 0.6344785690307617\n",
      "Loss and prediction by the model after 1060000 steps of gradient descent are 0.6794060468673706 0.9856218099594116\n",
      "Loss and prediction by the model after 1070000 steps of gradient descent are 0.6788406372070312 0.9647673964500427\n",
      "Loss and prediction by the model after 1080000 steps of gradient descent are 0.6782742142677307 0.9133294224739075\n",
      "Loss and prediction by the model after 1090000 steps of gradient descent are 0.6777010560035706 0.8937253355979919\n",
      "Loss and prediction by the model after 1100000 steps of gradient descent are 0.6771237850189209 0.9077464938163757\n",
      "Loss and prediction by the model after 1110000 steps of gradient descent are 0.6765308976173401 0.9989492297172546\n",
      "Loss and prediction by the model after 1120000 steps of gradient descent are 0.6760208010673523 0.7314907312393188\n",
      "Loss and prediction by the model after 1130000 steps of gradient descent are 0.6755035519599915 0.6183024644851685\n",
      "Loss and prediction by the model after 1140000 steps of gradient descent are 0.6748092770576477 0.9976562857627869\n",
      "Loss and prediction by the model after 1150000 steps of gradient descent are 0.6742372512817383 0.9993360638618469\n",
      "Loss and prediction by the model after 1160000 steps of gradient descent are 0.6736820936203003 0.9032147526741028\n",
      "Loss and prediction by the model after 1170000 steps of gradient descent are 0.6730751991271973 0.9981477856636047\n",
      "Loss and prediction by the model after 1180000 steps of gradient descent are 0.6725284457206726 0.8838565945625305\n",
      "Loss and prediction by the model after 1190000 steps of gradient descent are 0.6719356775283813 0.9999997615814209\n",
      "Loss and prediction by the model after 1200000 steps of gradient descent are 0.6713657975196838 0.9992033839225769\n",
      "Loss and prediction by the model after 1210000 steps of gradient descent are 0.6708028316497803 0.9458717107772827\n",
      "Loss and prediction by the model after 1220000 steps of gradient descent are 0.6702726483345032 0.7699203491210938\n",
      "Loss and prediction by the model after 1230000 steps of gradient descent are 0.6700445413589478 0.39808404445648193\n",
      "Loss and prediction by the model after 1240000 steps of gradient descent are 0.6690904498100281 0.9341011047363281\n",
      "Loss and prediction by the model after 1250000 steps of gradient descent are 0.6685047149658203 0.9999957084655762\n",
      "Loss and prediction by the model after 1260000 steps of gradient descent are 0.6681902408599854 0.47761979699134827\n",
      "Loss and prediction by the model after 1270000 steps of gradient descent are 0.6673653721809387 0.9946799278259277\n",
      "Loss and prediction by the model after 1280000 steps of gradient descent are 0.666790783405304 0.9998355507850647\n",
      "Loss and prediction by the model after 1290000 steps of gradient descent are 0.6662178039550781 0.9835015535354614\n",
      "Loss and prediction by the model after 1300000 steps of gradient descent are 0.6656410098075867 0.9962793588638306\n",
      "Loss and prediction by the model after 1310000 steps of gradient descent are 0.6666018962860107 0.21601636707782745\n",
      "Loss and prediction by the model after 1320000 steps of gradient descent are 0.6644981503486633 0.990472137928009\n",
      "Loss and prediction by the model after 1330000 steps of gradient descent are 0.6639212965965271 0.9997825026512146\n",
      "Loss and prediction by the model after 1340000 steps of gradient descent are 0.6633278727531433 0.994324266910553\n",
      "Loss and prediction by the model after 1350000 steps of gradient descent are 0.6627677083015442 0.937727689743042\n",
      "Loss and prediction by the model after 1360000 steps of gradient descent are 0.6622021198272705 0.8945056796073914\n",
      "Loss and prediction by the model after 1370000 steps of gradient descent are 0.6632764339447021 0.20818106830120087\n",
      "Loss and prediction by the model after 1380000 steps of gradient descent are 0.6610326766967773 0.9999316930770874\n",
      "Loss and prediction by the model after 1390000 steps of gradient descent are 0.6604558229446411 0.9999292492866516\n",
      "Loss and prediction by the model after 1400000 steps of gradient descent are 0.659882664680481 0.9999710321426392\n",
      "Loss and prediction by the model after 1410000 steps of gradient descent are 0.6593042016029358 0.9939801096916199\n",
      "Loss and prediction by the model after 1420000 steps of gradient descent are 0.658729076385498 0.9958778619766235\n",
      "Loss and prediction by the model after 1430000 steps of gradient descent are 0.6581755876541138 0.9674901366233826\n",
      "Loss and prediction by the model after 1440000 steps of gradient descent are 0.6576239466667175 0.9942354559898376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss and prediction by the model after 1450000 steps of gradient descent are 0.657039999961853 0.9999775290489197\n",
      "Loss and prediction by the model after 1460000 steps of gradient descent are 0.6564981341362 0.8376306295394897\n",
      "Loss and prediction by the model after 1470000 steps of gradient descent are 0.6558922529220581 0.9985346794128418\n",
      "Loss and prediction by the model after 1480000 steps of gradient descent are 0.6553246378898621 0.9510308504104614\n",
      "Loss and prediction by the model after 1490000 steps of gradient descent are 0.6547876596450806 0.7896304130554199\n",
      "Loss and prediction by the model after 1500000 steps of gradient descent are 0.6541703939437866 0.993952214717865\n",
      "Loss and prediction by the model after 1510000 steps of gradient descent are 0.6535972952842712 0.9764919281005859\n",
      "Loss and prediction by the model after 1520000 steps of gradient descent are 0.6530267000198364 0.9879735708236694\n",
      "Loss and prediction by the model after 1530000 steps of gradient descent are 0.6525159478187561 0.7549557089805603\n",
      "Loss and prediction by the model after 1540000 steps of gradient descent are 0.6518868803977966 0.9623687863349915\n",
      "Loss and prediction by the model after 1550000 steps of gradient descent are 0.6513084173202515 0.9975086450576782\n",
      "Loss and prediction by the model after 1560000 steps of gradient descent are 0.6507223844528198 1.0\n",
      "Loss and prediction by the model after 1570000 steps of gradient descent are 0.6501761078834534 0.8487586379051208\n",
      "Loss and prediction by the model after 1580000 steps of gradient descent are 0.6495682597160339 0.9989625215530396\n",
      "Loss and prediction by the model after 1590000 steps of gradient descent are 0.6489976048469543 0.98431795835495\n",
      "Loss and prediction by the model after 1600000 steps of gradient descent are 0.6484194993972778 0.9995685815811157\n",
      "Loss and prediction by the model after 1610000 steps of gradient descent are 0.6478460431098938 0.9999724626541138\n",
      "Loss and prediction by the model after 1620000 steps of gradient descent are 0.6474022269248962 0.606283962726593\n",
      "Loss and prediction by the model after 1630000 steps of gradient descent are 0.646692156791687 0.9998624324798584\n",
      "Loss and prediction by the model after 1640000 steps of gradient descent are 0.6461265087127686 0.9421555995941162\n",
      "Loss and prediction by the model after 1650000 steps of gradient descent are 0.6455605626106262 0.8984971642494202\n",
      "Loss and prediction by the model after 1660000 steps of gradient descent are 0.6449602842330933 0.9998281002044678\n",
      "Loss and prediction by the model after 1670000 steps of gradient descent are 0.6443936228752136 0.9678237438201904\n",
      "Loss and prediction by the model after 1680000 steps of gradient descent are 0.6438179016113281 0.9968245625495911\n",
      "Loss and prediction by the model after 1690000 steps of gradient descent are 0.6433966159820557 0.5797250270843506\n",
      "Loss and prediction by the model after 1700000 steps of gradient descent are 0.6426688432693481 0.9873573184013367\n",
      "Loss and prediction by the model after 1710000 steps of gradient descent are 0.6420924663543701 0.9998461008071899\n",
      "Loss and prediction by the model after 1720000 steps of gradient descent are 0.6415221095085144 0.9999589920043945\n",
      "Loss and prediction by the model after 1730000 steps of gradient descent are 0.6409491896629333 0.9943434000015259\n",
      "Loss and prediction by the model after 1740000 steps of gradient descent are 0.6403794884681702 0.9828726649284363\n",
      "Loss and prediction by the model after 1750000 steps of gradient descent are 0.6398152112960815 0.9412727355957031\n",
      "Loss and prediction by the model after 1760000 steps of gradient descent are 0.6392325162887573 0.9983978271484375\n",
      "Loss and prediction by the model after 1770000 steps of gradient descent are 0.6386603713035583 0.9996772408485413\n",
      "Loss and prediction by the model after 1780000 steps of gradient descent are 0.6390062570571899 0.28022801876068115\n",
      "Loss and prediction by the model after 1790000 steps of gradient descent are 0.6375153064727783 0.9925426244735718\n",
      "Loss and prediction by the model after 1800000 steps of gradient descent are 0.6383119821548462 0.23232248425483704\n",
      "Loss and prediction by the model after 1810000 steps of gradient descent are 0.6363778114318848 0.9347100257873535\n",
      "Loss and prediction by the model after 1820000 steps of gradient descent are 0.6359091401100159 0.6365605592727661\n",
      "Loss and prediction by the model after 1830000 steps of gradient descent are 0.6354699730873108 0.4890778660774231\n",
      "Loss and prediction by the model after 1840000 steps of gradient descent are 0.6346437335014343 0.9993060827255249\n",
      "Loss and prediction by the model after 1850000 steps of gradient descent are 0.6341168284416199 0.7872239351272583\n",
      "Loss and prediction by the model after 1860000 steps of gradient descent are 0.633495569229126 0.9978605508804321\n",
      "Loss and prediction by the model after 1870000 steps of gradient descent are 0.6329460740089417 0.8762617111206055\n",
      "Loss and prediction by the model after 1880000 steps of gradient descent are 0.6323504447937012 0.9999492764472961\n",
      "Loss and prediction by the model after 1890000 steps of gradient descent are 0.6318116784095764 0.8368209004402161\n",
      "Loss and prediction by the model after 1900000 steps of gradient descent are 0.6315228343009949 0.44507884979248047\n",
      "Loss and prediction by the model after 1910000 steps of gradient descent are 0.6306336522102356 0.9772759079933167\n",
      "Loss and prediction by the model after 1920000 steps of gradient descent are 0.6300576329231262 0.9998005628585815\n",
      "Loss and prediction by the model after 1930000 steps of gradient descent are 0.6294872164726257 0.9996896386146545\n",
      "Loss and prediction by the model after 1940000 steps of gradient descent are 0.628915548324585 0.9998053908348083\n",
      "Loss and prediction by the model after 1950000 steps of gradient descent are 0.628342866897583 0.9853429198265076\n",
      "Loss and prediction by the model after 1960000 steps of gradient descent are 0.6277748346328735 0.9805100560188293\n",
      "Loss and prediction by the model after 1970000 steps of gradient descent are 0.6274530291557312 0.487575501203537\n",
      "Loss and prediction by the model after 1980000 steps of gradient descent are 0.6266264915466309 0.9903080463409424\n",
      "Loss and prediction by the model after 1990000 steps of gradient descent are 0.6260544657707214 0.9996734261512756\n",
      "Loss and prediction by the model after 2000000 steps of gradient descent are 0.6254844665527344 0.9970124959945679\n",
      "Loss and prediction by the model after 2010000 steps of gradient descent are 0.6249102354049683 0.9992083311080933\n",
      "Loss and prediction by the model after 2020000 steps of gradient descent are 0.6244233250617981 0.7120623588562012\n",
      "Loss and prediction by the model after 2030000 steps of gradient descent are 0.6237736344337463 0.9990860223770142\n",
      "Loss and prediction by the model after 2040000 steps of gradient descent are 0.623203456401825 0.9930572509765625\n",
      "Loss and prediction by the model after 2050000 steps of gradient descent are 0.6226310133934021 0.9998264312744141\n",
      "Loss and prediction by the model after 2060000 steps of gradient descent are 0.6220661997795105 0.9989446997642517\n",
      "Loss and prediction by the model after 2070000 steps of gradient descent are 0.621526837348938 0.8382261991500854\n",
      "Loss and prediction by the model after 2080000 steps of gradient descent are 0.6209231019020081 0.9905096888542175\n",
      "Loss and prediction by the model after 2090000 steps of gradient descent are 0.6205329895019531 0.5513511300086975\n",
      "Loss and prediction by the model after 2100000 steps of gradient descent are 0.619801938533783 0.8649497628211975\n",
      "Loss and prediction by the model after 2110000 steps of gradient descent are 0.6192118525505066 0.9789799451828003\n",
      "Loss and prediction by the model after 2120000 steps of gradient descent are 0.6186429262161255 0.9698408842086792\n",
      "Loss and prediction by the model after 2130000 steps of gradient descent are 0.618080735206604 0.923313558101654\n",
      "Loss and prediction by the model after 2140000 steps of gradient descent are 0.617498517036438 0.9910067915916443\n",
      "Loss and prediction by the model after 2150000 steps of gradient descent are 0.6169208884239197 0.9958220720291138\n",
      "Loss and prediction by the model after 2160000 steps of gradient descent are 0.6163538098335266 0.9525488018989563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss and prediction by the model after 2170000 steps of gradient descent are 0.6157715916633606 0.9719710350036621\n",
      "Loss and prediction by the model after 2180000 steps of gradient descent are 0.6151958703994751 0.9727122783660889\n",
      "Loss and prediction by the model after 2190000 steps of gradient descent are 0.6146228313446045 0.9535028338432312\n",
      "Loss and prediction by the model after 2200000 steps of gradient descent are 0.6140499711036682 0.9558911323547363\n",
      "Loss and prediction by the model after 2210000 steps of gradient descent are 0.6134644746780396 0.9998538494110107\n",
      "Loss and prediction by the model after 2220000 steps of gradient descent are 0.6130626797676086 0.5641770362854004\n",
      "Loss and prediction by the model after 2230000 steps of gradient descent are 0.6123190522193909 0.9986355304718018\n",
      "Loss and prediction by the model after 2240000 steps of gradient descent are 0.6117417812347412 0.9999946355819702\n",
      "Loss and prediction by the model after 2250000 steps of gradient descent are 0.6111606955528259 0.9973574876785278\n",
      "Loss and prediction by the model after 2260000 steps of gradient descent are 0.610584557056427 0.9999984502792358\n",
      "Loss and prediction by the model after 2270000 steps of gradient descent are 0.6100118160247803 0.9999815821647644\n",
      "Loss and prediction by the model after 2280000 steps of gradient descent are 0.6094374060630798 0.9775996804237366\n",
      "Loss and prediction by the model after 2290000 steps of gradient descent are 0.8835746645927429 0.015615488402545452\n",
      "Loss and prediction by the model after 2300000 steps of gradient descent are 0.6083113551139832 0.855636715888977\n",
      "Loss and prediction by the model after 2310000 steps of gradient descent are 0.6078423857688904 0.6075499057769775\n",
      "Loss and prediction by the model after 2320000 steps of gradient descent are 0.607127845287323 0.9999666810035706\n",
      "Loss and prediction by the model after 2330000 steps of gradient descent are 0.6065504550933838 0.9999998807907104\n",
      "Loss and prediction by the model after 2340000 steps of gradient descent are 0.6059921383857727 0.9045299887657166\n",
      "Loss and prediction by the model after 2350000 steps of gradient descent are 0.6053985953330994 0.9997277855873108\n",
      "Loss and prediction by the model after 2360000 steps of gradient descent are 0.60482257604599 0.9998753666877747\n",
      "Loss and prediction by the model after 2370000 steps of gradient descent are 0.6042458415031433 0.9999936819076538\n",
      "Loss and prediction by the model after 2380000 steps of gradient descent are 0.6036661267280579 0.9995185732841492\n",
      "Loss and prediction by the model after 2390000 steps of gradient descent are 0.6030945777893066 0.9969707727432251\n",
      "Loss and prediction by the model after 2400000 steps of gradient descent are 0.6025174856185913 0.9999986886978149\n",
      "Loss and prediction by the model after 2410000 steps of gradient descent are 0.6019383072853088 0.9972586035728455\n",
      "Loss and prediction by the model after 2420000 steps of gradient descent are 0.6013601422309875 0.9998393058776855\n",
      "Loss and prediction by the model after 2430000 steps of gradient descent are 0.6007844805717468 0.9999971985816956\n",
      "Loss and prediction by the model after 2440000 steps of gradient descent are 0.6002102494239807 0.9979438185691833\n",
      "Loss and prediction by the model after 2450000 steps of gradient descent are 0.5996341109275818 0.9564956426620483\n",
      "Loss and prediction by the model after 2460000 steps of gradient descent are 0.5990739464759827 0.990227460861206\n",
      "Loss and prediction by the model after 2470000 steps of gradient descent are 0.5984902381896973 0.9996108412742615\n",
      "Loss and prediction by the model after 2480000 steps of gradient descent are 0.597920298576355 0.9690331220626831\n",
      "Loss and prediction by the model after 2490000 steps of gradient descent are 0.5973433256149292 0.9637532234191895\n",
      "Loss and prediction by the model after 2500000 steps of gradient descent are 0.5967856049537659 0.9999729990959167\n",
      "Loss and prediction by the model after 2510000 steps of gradient descent are 0.6028023362159729 0.1117682158946991\n",
      "Loss and prediction by the model after 2520000 steps of gradient descent are 0.5956287980079651 0.9994641542434692\n",
      "Loss and prediction by the model after 2530000 steps of gradient descent are 0.5950488448143005 0.9967545866966248\n",
      "Loss and prediction by the model after 2540000 steps of gradient descent are 0.5944716334342957 0.9884163737297058\n",
      "Loss and prediction by the model after 2550000 steps of gradient descent are 0.5938884615898132 0.9998471140861511\n",
      "Loss and prediction by the model after 2560000 steps of gradient descent are 0.593305230140686 0.999636709690094\n",
      "Loss and prediction by the model after 2570000 steps of gradient descent are 0.5927241444587708 0.998623788356781\n",
      "Loss and prediction by the model after 2580000 steps of gradient descent are 0.5921452045440674 0.9998385906219482\n",
      "Loss and prediction by the model after 2590000 steps of gradient descent are 0.5915663838386536 0.9900732040405273\n",
      "Loss and prediction by the model after 2600000 steps of gradient descent are 0.5912359356880188 0.5009618997573853\n",
      "Loss and prediction by the model after 2610000 steps of gradient descent are 0.5906176567077637 0.535281240940094\n",
      "Loss and prediction by the model after 2620000 steps of gradient descent are 0.5898364782333374 0.9979662895202637\n",
      "Loss and prediction by the model after 2630000 steps of gradient descent are 0.5892611145973206 0.9982913732528687\n",
      "Loss and prediction by the model after 2640000 steps of gradient descent are 0.588683009147644 0.9999352693557739\n",
      "Loss and prediction by the model after 2650000 steps of gradient descent are 0.5881083607673645 0.9999682903289795\n",
      "Loss and prediction by the model after 2660000 steps of gradient descent are 0.5875347256660461 0.999770998954773\n",
      "Loss and prediction by the model after 2670000 steps of gradient descent are 0.5869705080986023 0.913637101650238\n",
      "Loss and prediction by the model after 2680000 steps of gradient descent are 0.5863929986953735 0.8894968032836914\n",
      "Loss and prediction by the model after 2690000 steps of gradient descent are 0.5857933759689331 0.9999727010726929\n",
      "Loss and prediction by the model after 2700000 steps of gradient descent are 0.5852132439613342 0.9960836172103882\n",
      "Loss and prediction by the model after 2710000 steps of gradient descent are 0.5846326351165771 0.995441198348999\n",
      "Loss and prediction by the model after 2720000 steps of gradient descent are 0.6007466316223145 0.07092160731554031\n",
      "Loss and prediction by the model after 2730000 steps of gradient descent are 0.5834854245185852 0.9985608458518982\n",
      "Loss and prediction by the model after 2740000 steps of gradient descent are 0.5845807194709778 0.22041763365268707\n",
      "Loss and prediction by the model after 2750000 steps of gradient descent are 0.5823308229446411 0.9979398250579834\n",
      "Loss and prediction by the model after 2760000 steps of gradient descent are 0.5817695260047913 0.9081067442893982\n",
      "Loss and prediction by the model after 2770000 steps of gradient descent are 0.5812236666679382 0.780076265335083\n",
      "Loss and prediction by the model after 2780000 steps of gradient descent are 0.5810804963111877 0.38578999042510986\n",
      "Loss and prediction by the model after 2790000 steps of gradient descent are 0.5800075531005859 0.9999533295631409\n",
      "Loss and prediction by the model after 2800000 steps of gradient descent are 0.5795614719390869 0.6289111971855164\n",
      "Loss and prediction by the model after 2810000 steps of gradient descent are 0.5788562297821045 0.9594660997390747\n",
      "Loss and prediction by the model after 2820000 steps of gradient descent are 0.5782738327980042 0.9999979734420776\n",
      "Loss and prediction by the model after 2830000 steps of gradient descent are 0.578058123588562 0.4369353950023651\n",
      "Loss and prediction by the model after 2840000 steps of gradient descent are 0.5771180987358093 0.9997556209564209\n",
      "Loss and prediction by the model after 2850000 steps of gradient descent are 0.576551616191864 0.929531455039978\n",
      "Loss and prediction by the model after 2860000 steps of gradient descent are 0.575973629951477 0.9926478266716003\n",
      "Loss and prediction by the model after 2870000 steps of gradient descent are 0.5754004120826721 0.9932447671890259\n",
      "Loss and prediction by the model after 2880000 steps of gradient descent are 0.5748220682144165 0.9989330172538757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss and prediction by the model after 2890000 steps of gradient descent are 0.5742842555046082 0.8131111264228821\n",
      "Loss and prediction by the model after 2900000 steps of gradient descent are 0.5736806988716125 0.9033458232879639\n",
      "Loss and prediction by the model after 2910000 steps of gradient descent are 0.5731051564216614 0.8969506621360779\n",
      "Loss and prediction by the model after 2920000 steps of gradient descent are 0.5725113749504089 0.9719971418380737\n",
      "Loss and prediction by the model after 2930000 steps of gradient descent are 0.5719289183616638 0.9982343912124634\n",
      "Loss and prediction by the model after 2940000 steps of gradient descent are 0.5713484883308411 0.996943473815918\n",
      "Loss and prediction by the model after 2950000 steps of gradient descent are 0.570774257183075 0.9837610125541687\n",
      "Loss and prediction by the model after 2960000 steps of gradient descent are 0.5701913237571716 0.9954717755317688\n",
      "Loss and prediction by the model after 2970000 steps of gradient descent are 0.5696167349815369 0.9997451305389404\n",
      "Loss and prediction by the model after 2980000 steps of gradient descent are 0.5690411925315857 0.9939998984336853\n",
      "Loss and prediction by the model after 2990000 steps of gradient descent are 0.5684617757797241 0.9997439980506897\n",
      "Loss and prediction by the model after 3000000 steps of gradient descent are 0.567889392375946 0.9928274750709534\n",
      "Loss and prediction by the model after 3010000 steps of gradient descent are 0.567312479019165 0.970251739025116\n",
      "Loss and prediction by the model after 3020000 steps of gradient descent are 0.5667375326156616 0.9967777132987976\n",
      "Loss and prediction by the model after 3030000 steps of gradient descent are 0.5687475800514221 0.1814718097448349\n",
      "Loss and prediction by the model after 3040000 steps of gradient descent are 0.567279577255249 0.2221553921699524\n",
      "Loss and prediction by the model after 3050000 steps of gradient descent are 0.5650820732116699 0.7178913354873657\n",
      "Loss and prediction by the model after 3060000 steps of gradient descent are 0.5644375681877136 0.9528332948684692\n",
      "Loss and prediction by the model after 3070000 steps of gradient descent are 0.563852071762085 0.9978471994400024\n",
      "Loss and prediction by the model after 3080000 steps of gradient descent are 0.5632779598236084 0.9989807605743408\n",
      "Loss and prediction by the model after 3090000 steps of gradient descent are 0.5650172233581543 0.1921594887971878\n",
      "Loss and prediction by the model after 3100000 steps of gradient descent are 0.5621615052223206 0.8572613596916199\n",
      "Loss and prediction by the model after 3110000 steps of gradient descent are 0.5615665316581726 0.9971612095832825\n",
      "Loss and prediction by the model after 3120000 steps of gradient descent are 0.5609987378120422 0.9518887996673584\n",
      "Loss and prediction by the model after 3130000 steps of gradient descent are 0.5604937672615051 0.7281033992767334\n",
      "Loss and prediction by the model after 3140000 steps of gradient descent are 0.5598410367965698 0.9994818568229675\n",
      "Loss and prediction by the model after 3150000 steps of gradient descent are 0.5593135952949524 0.8133395910263062\n",
      "Loss and prediction by the model after 3160000 steps of gradient descent are 0.5587183237075806 0.8856288194656372\n",
      "Loss and prediction by the model after 3170000 steps of gradient descent are 0.5581182241439819 0.9859693646430969\n",
      "Loss and prediction by the model after 3180000 steps of gradient descent are 0.5575495958328247 0.9825505614280701\n",
      "Loss and prediction by the model after 3190000 steps of gradient descent are 0.5569724440574646 0.998699963092804\n",
      "Loss and prediction by the model after 3200000 steps of gradient descent are 0.556394636631012 0.9970889091491699\n",
      "Loss and prediction by the model after 3210000 steps of gradient descent are 0.5560531616210938 0.5319807529449463\n",
      "Loss and prediction by the model after 3220000 steps of gradient descent are 0.5555516481399536 0.47675028443336487\n",
      "Loss and prediction by the model after 3230000 steps of gradient descent are 0.5546724200248718 0.9780250787734985\n",
      "Loss and prediction by the model after 3240000 steps of gradient descent are 0.5541072487831116 0.999951183795929\n",
      "Loss and prediction by the model after 3250000 steps of gradient descent are 0.5535341501235962 0.9995693564414978\n",
      "Loss and prediction by the model after 3260000 steps of gradient descent are 0.5529619455337524 0.9999457597732544\n",
      "Loss and prediction by the model after 3270000 steps of gradient descent are 0.5523920059204102 0.9823328256607056\n",
      "Loss and prediction by the model after 3280000 steps of gradient descent are 1.693556785583496 0.006245444528758526\n",
      "Loss and prediction by the model after 3290000 steps of gradient descent are 0.5512456297874451 0.9778128862380981\n",
      "Loss and prediction by the model after 3300000 steps of gradient descent are 0.5506728291511536 0.9944215416908264\n",
      "Loss and prediction by the model after 3310000 steps of gradient descent are 0.550134539604187 0.8346664905548096\n",
      "Loss and prediction by the model after 3320000 steps of gradient descent are 0.5495471954345703 0.9039105176925659\n",
      "Loss and prediction by the model after 3330000 steps of gradient descent are 0.5489489436149597 0.9996234178543091\n",
      "Loss and prediction by the model after 3340000 steps of gradient descent are 0.5487470626831055 0.44682297110557556\n",
      "Loss and prediction by the model after 3350000 steps of gradient descent are 0.5481323003768921 0.4679068326950073\n",
      "Loss and prediction by the model after 3360000 steps of gradient descent are 0.5472477674484253 0.9530919790267944\n",
      "Loss and prediction by the model after 3370000 steps of gradient descent are 0.5470532774925232 0.43626898527145386\n",
      "Loss and prediction by the model after 3380000 steps of gradient descent are 0.5460922718048096 0.9785903692245483\n",
      "Loss and prediction by the model after 3390000 steps of gradient descent are 0.5455104112625122 0.9766038656234741\n",
      "Loss and prediction by the model after 3400000 steps of gradient descent are 0.5449314713478088 0.9999982118606567\n",
      "Loss and prediction by the model after 3410000 steps of gradient descent are 0.5443628430366516 0.9878630042076111\n",
      "Loss and prediction by the model after 3420000 steps of gradient descent are 0.5437852740287781 0.9998094439506531\n",
      "Loss and prediction by the model after 3430000 steps of gradient descent are 0.5432208180427551 0.9989869594573975\n",
      "Loss and prediction by the model after 3440000 steps of gradient descent are 0.542676568031311 0.8711055517196655\n",
      "Loss and prediction by the model after 3450000 steps of gradient descent are 0.5420802235603333 0.9993419647216797\n",
      "Loss and prediction by the model after 3460000 steps of gradient descent are 0.541513979434967 0.9658695459365845\n",
      "Loss and prediction by the model after 3470000 steps of gradient descent are 0.5409383177757263 0.998975396156311\n",
      "Loss and prediction by the model after 3480000 steps of gradient descent are 0.5403711795806885 0.9997977018356323\n",
      "Loss and prediction by the model after 3490000 steps of gradient descent are 0.5397983193397522 0.9999699592590332\n",
      "Loss and prediction by the model after 3500000 steps of gradient descent are 0.5392380952835083 0.9811423420906067\n",
      "Loss and prediction by the model after 3510000 steps of gradient descent are 0.5386718511581421 0.958640456199646\n",
      "Loss and prediction by the model after 3520000 steps of gradient descent are 0.5380938053131104 0.9991970062255859\n",
      "Loss and prediction by the model after 3530000 steps of gradient descent are 0.5375206470489502 0.9982699751853943\n",
      "Loss and prediction by the model after 3540000 steps of gradient descent are 0.5369476079940796 0.9999372959136963\n",
      "Loss and prediction by the model after 3550000 steps of gradient descent are 0.8182496428489685 0.016180984675884247\n",
      "Loss and prediction by the model after 3560000 steps of gradient descent are 0.5358021855354309 0.9977301359176636\n",
      "Loss and prediction by the model after 3570000 steps of gradient descent are 0.5352373123168945 0.9955977201461792\n",
      "Loss and prediction by the model after 3580000 steps of gradient descent are 0.5346872806549072 0.9164208769798279\n",
      "Loss and prediction by the model after 3590000 steps of gradient descent are 0.5340979099273682 0.9980387687683105\n",
      "Loss and prediction by the model after 3600000 steps of gradient descent are 0.5335422158241272 0.9778464436531067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss and prediction by the model after 3610000 steps of gradient descent are 0.5330389142036438 0.7579405903816223\n",
      "Loss and prediction by the model after 3620000 steps of gradient descent are 0.5324264168739319 0.8688399791717529\n",
      "Loss and prediction by the model after 3630000 steps of gradient descent are 0.5322825908660889 0.41208121180534363\n",
      "Loss and prediction by the model after 3640000 steps of gradient descent are 0.53131103515625 0.7886173129081726\n",
      "Loss and prediction by the model after 3650000 steps of gradient descent are 0.5306892991065979 0.9928259253501892\n",
      "Loss and prediction by the model after 3660000 steps of gradient descent are 0.5301222801208496 0.9998748302459717\n",
      "Loss and prediction by the model after 3670000 steps of gradient descent are 0.5295486450195312 0.9992870092391968\n",
      "Loss and prediction by the model after 3680000 steps of gradient descent are 0.5289809703826904 0.9959454536437988\n",
      "Loss and prediction by the model after 3690000 steps of gradient descent are 0.5284339785575867 0.8896631598472595\n",
      "Loss and prediction by the model after 3700000 steps of gradient descent are 0.5283632874488831 0.39042791724205017\n",
      "Loss and prediction by the model after 3710000 steps of gradient descent are 0.5273271203041077 0.7778111696243286\n",
      "Loss and prediction by the model after 3720000 steps of gradient descent are 0.5267266035079956 0.8766742944717407\n",
      "Loss and prediction by the model after 3730000 steps of gradient descent are 0.5261290669441223 0.9968756437301636\n",
      "Loss and prediction by the model after 3740000 steps of gradient descent are 0.5255547761917114 0.999897837638855\n",
      "Loss and prediction by the model after 3750000 steps of gradient descent are 0.524993896484375 0.9911078810691833\n",
      "Loss and prediction by the model after 3760000 steps of gradient descent are 0.5244195461273193 0.9856966733932495\n",
      "Loss and prediction by the model after 3770000 steps of gradient descent are 0.5238468647003174 0.9999748468399048\n",
      "Loss and prediction by the model after 3780000 steps of gradient descent are 0.5232858061790466 0.9991244673728943\n",
      "Loss and prediction by the model after 3790000 steps of gradient descent are 0.5277571082115173 0.13617831468582153\n",
      "Loss and prediction by the model after 3800000 steps of gradient descent are 0.5221672058105469 0.9069617986679077\n",
      "Loss and prediction by the model after 3810000 steps of gradient descent are 0.5215861797332764 0.9521480202674866\n",
      "Loss and prediction by the model after 3820000 steps of gradient descent are 0.5210230350494385 0.9657725691795349\n",
      "Loss and prediction by the model after 3830000 steps of gradient descent are 0.5204490423202515 0.9844067096710205\n",
      "Loss and prediction by the model after 3840000 steps of gradient descent are 0.5198863744735718 0.9877875447273254\n",
      "Loss and prediction by the model after 3850000 steps of gradient descent are 0.5193504095077515 0.8807477951049805\n",
      "Loss and prediction by the model after 3860000 steps of gradient descent are 0.518757164478302 0.9996386766433716\n",
      "Loss and prediction by the model after 3870000 steps of gradient descent are 0.5181886553764343 0.998857855796814\n",
      "Loss and prediction by the model after 3880000 steps of gradient descent are 0.5177361369132996 0.6675407290458679\n",
      "Loss and prediction by the model after 3890000 steps of gradient descent are 0.517078161239624 0.8796240091323853\n",
      "Loss and prediction by the model after 3900000 steps of gradient descent are 0.5164845585823059 0.9924874305725098\n",
      "Loss and prediction by the model after 3910000 steps of gradient descent are 0.5159125328063965 0.9983171224594116\n",
      "Loss and prediction by the model after 3920000 steps of gradient descent are 0.5153504014015198 0.9817569255828857\n",
      "Loss and prediction by the model after 3930000 steps of gradient descent are 0.5147801637649536 0.9983258247375488\n",
      "Loss and prediction by the model after 3940000 steps of gradient descent are 0.5142514109611511 0.8489125967025757\n",
      "Loss and prediction by the model after 3950000 steps of gradient descent are 0.5136515498161316 0.9999263286590576\n",
      "Loss and prediction by the model after 3960000 steps of gradient descent are 0.5131329298019409 0.8118329644203186\n",
      "Loss and prediction by the model after 3970000 steps of gradient descent are 0.5125152468681335 0.9977185130119324\n",
      "Loss and prediction by the model after 3980000 steps of gradient descent are 0.5119573473930359 0.9994932413101196\n",
      "Loss and prediction by the model after 3990000 steps of gradient descent are 0.5113834738731384 0.9989275932312012\n",
      "Loss and prediction by the model after 4000000 steps of gradient descent are 0.5124046802520752 0.24118292331695557\n",
      "Loss and prediction by the model after 4010000 steps of gradient descent are 0.510279655456543 0.9135247468948364\n",
      "Loss and prediction by the model after 4020000 steps of gradient descent are 0.50969398021698 0.9997885227203369\n",
      "Loss and prediction by the model after 4030000 steps of gradient descent are 0.509215772151947 0.7335997819900513\n",
      "Loss and prediction by the model after 4040000 steps of gradient descent are 0.5085771083831787 0.9997283220291138\n",
      "Loss and prediction by the model after 4050000 steps of gradient descent are 0.5080468058586121 0.9175332188606262\n",
      "Loss and prediction by the model after 4060000 steps of gradient descent are 0.5074711441993713 0.9871902465820312\n",
      "Loss and prediction by the model after 4070000 steps of gradient descent are 0.5073390007019043 0.4367643892765045\n",
      "Loss and prediction by the model after 4080000 steps of gradient descent are 0.5063665509223938 0.9915363192558289\n",
      "Loss and prediction by the model after 4090000 steps of gradient descent are 0.5058338046073914 0.8524998426437378\n",
      "Loss and prediction by the model after 4100000 steps of gradient descent are 0.505393385887146 0.6055876016616821\n",
      "Loss and prediction by the model after 4110000 steps of gradient descent are 0.5047519207000732 0.7099819779396057\n",
      "Loss and prediction by the model after 4120000 steps of gradient descent are 0.5042024254798889 0.7458093166351318\n",
      "Loss and prediction by the model after 4130000 steps of gradient descent are 0.5035908222198486 0.9995055198669434\n",
      "Loss and prediction by the model after 4140000 steps of gradient descent are 0.5031157732009888 0.7133639454841614\n",
      "Loss and prediction by the model after 4150000 steps of gradient descent are 0.5024556517601013 0.9947415590286255\n",
      "Loss and prediction by the model after 4160000 steps of gradient descent are 0.5024282336235046 0.39562922716140747\n",
      "Loss and prediction by the model after 4170000 steps of gradient descent are 0.5013370513916016 0.9591594338417053\n",
      "Loss and prediction by the model after 4180000 steps of gradient descent are 0.5007658004760742 0.9990857243537903\n",
      "Loss and prediction by the model after 4190000 steps of gradient descent are 0.5002056360244751 0.9994099736213684\n",
      "Loss and prediction by the model after 4200000 steps of gradient descent are 0.4996488690376282 0.9802832007408142\n",
      "Loss and prediction by the model after 4210000 steps of gradient descent are 0.4990774393081665 0.9993528723716736\n",
      "Loss and prediction by the model after 4220000 steps of gradient descent are 0.4985295236110687 0.9482114911079407\n",
      "Loss and prediction by the model after 4230000 steps of gradient descent are 0.4980008602142334 0.824084997177124\n",
      "Loss and prediction by the model after 4240000 steps of gradient descent are 0.49744388461112976 0.8168464303016663\n",
      "Loss and prediction by the model after 4250000 steps of gradient descent are 0.4968372583389282 0.9930364489555359\n",
      "Loss and prediction by the model after 4260000 steps of gradient descent are 0.49628138542175293 0.9991412162780762\n",
      "Loss and prediction by the model after 4270000 steps of gradient descent are 0.4958561658859253 0.6486112475395203\n",
      "Loss and prediction by the model after 4280000 steps of gradient descent are 0.4951705038547516 0.9643623232841492\n",
      "Loss and prediction by the model after 4290000 steps of gradient descent are 0.4945961534976959 0.9865980744361877\n",
      "Loss and prediction by the model after 4300000 steps of gradient descent are 0.49545830488204956 0.2571287155151367\n",
      "Loss and prediction by the model after 4310000 steps of gradient descent are 0.5004507303237915 0.1192372739315033\n",
      "Loss and prediction by the model after 4320000 steps of gradient descent are 0.49292221665382385 0.9325339198112488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss and prediction by the model after 4330000 steps of gradient descent are 0.4923497140407562 0.9939544200897217\n",
      "Loss and prediction by the model after 4340000 steps of gradient descent are 0.49179452657699585 0.9992972016334534\n",
      "Loss and prediction by the model after 4350000 steps of gradient descent are 0.4922948181629181 0.294891893863678\n",
      "Loss and prediction by the model after 4360000 steps of gradient descent are 0.49070921540260315 0.8720273375511169\n",
      "Loss and prediction by the model after 4370000 steps of gradient descent are 0.49011820554733276 0.9969767928123474\n",
      "Loss and prediction by the model after 4380000 steps of gradient descent are 0.48955145478248596 0.9988832473754883\n",
      "Loss and prediction by the model after 4390000 steps of gradient descent are 0.4889923334121704 0.9999376535415649\n",
      "Loss and prediction by the model after 4400000 steps of gradient descent are 0.4884398579597473 0.9989058971405029\n",
      "Loss and prediction by the model after 4410000 steps of gradient descent are 0.48790448904037476 0.9993816018104553\n",
      "Loss and prediction by the model after 4420000 steps of gradient descent are 0.4873684048652649 0.9998716115951538\n",
      "Loss and prediction by the model after 4430000 steps of gradient descent are 0.4868200123310089 0.9956257343292236\n",
      "Loss and prediction by the model after 4440000 steps of gradient descent are 0.48627275228500366 0.995600700378418\n",
      "Loss and prediction by the model after 4450000 steps of gradient descent are 0.4857272505760193 0.9519211649894714\n",
      "Loss and prediction by the model after 4460000 steps of gradient descent are 0.48516184091567993 0.9868987798690796\n",
      "Loss and prediction by the model after 4470000 steps of gradient descent are 0.4846023619174957 0.9984343647956848\n",
      "Loss and prediction by the model after 4480000 steps of gradient descent are 0.4840478301048279 0.9998931884765625\n",
      "Loss and prediction by the model after 4490000 steps of gradient descent are 0.483568012714386 0.7392266392707825\n",
      "Loss and prediction by the model after 4500000 steps of gradient descent are 0.482957124710083 0.8739953637123108\n",
      "Loss and prediction by the model after 4510000 steps of gradient descent are 0.48240646719932556 0.9999548196792603\n",
      "Loss and prediction by the model after 4520000 steps of gradient descent are 0.48184436559677124 0.9812966585159302\n",
      "Loss and prediction by the model after 4530000 steps of gradient descent are 0.4812847077846527 0.9776268005371094\n",
      "Loss and prediction by the model after 4540000 steps of gradient descent are 0.480724036693573 0.9848682880401611\n",
      "Loss and prediction by the model after 4550000 steps of gradient descent are 0.4802284240722656 0.7845807671546936\n",
      "Loss and prediction by the model after 4560000 steps of gradient descent are 0.47960570454597473 0.9999021291732788\n",
      "Loss and prediction by the model after 4570000 steps of gradient descent are 0.4790518879890442 0.965124249458313\n",
      "Loss and prediction by the model after 4580000 steps of gradient descent are 0.4784983992576599 0.9617693424224854\n",
      "Loss and prediction by the model after 4590000 steps of gradient descent are 0.4779357612133026 0.9950344562530518\n",
      "Loss and prediction by the model after 4600000 steps of gradient descent are 0.47738274931907654 0.9986720681190491\n",
      "Loss and prediction by the model after 4610000 steps of gradient descent are 0.47698432207107544 0.636170506477356\n",
      "Loss and prediction by the model after 4620000 steps of gradient descent are 0.47627270221710205 0.987882137298584\n",
      "Loss and prediction by the model after 4630000 steps of gradient descent are 0.4845711588859558 0.10786078870296478\n",
      "Loss and prediction by the model after 4640000 steps of gradient descent are 0.4751950204372406 0.908811628818512\n",
      "Loss and prediction by the model after 4650000 steps of gradient descent are 0.4747302234172821 0.6971786618232727\n",
      "Loss and prediction by the model after 4660000 steps of gradient descent are 0.4740898311138153 0.9397169351577759\n",
      "Loss and prediction by the model after 4670000 steps of gradient descent are 0.4735223352909088 0.975757360458374\n",
      "Loss and prediction by the model after 4680000 steps of gradient descent are 0.4729585349559784 0.9941345453262329\n",
      "Loss and prediction by the model after 4690000 steps of gradient descent are 0.4723946452140808 0.9994716048240662\n",
      "Loss and prediction by the model after 4700000 steps of gradient descent are 0.47185587882995605 0.9715086817741394\n",
      "Loss and prediction by the model after 4710000 steps of gradient descent are 0.47131654620170593 0.9172370433807373\n",
      "Loss and prediction by the model after 4720000 steps of gradient descent are 0.47217366099357605 0.26219138503074646\n",
      "Loss and prediction by the model after 4730000 steps of gradient descent are 0.47020673751831055 0.933853805065155\n",
      "Loss and prediction by the model after 4740000 steps of gradient descent are 0.469632625579834 0.9978466033935547\n",
      "Loss and prediction by the model after 4750000 steps of gradient descent are 0.46912136673927307 0.8443060517311096\n",
      "Loss and prediction by the model after 4760000 steps of gradient descent are 0.46852681040763855 0.9613614082336426\n",
      "Loss and prediction by the model after 4770000 steps of gradient descent are 0.4679636061191559 0.9986244440078735\n",
      "Loss and prediction by the model after 4780000 steps of gradient descent are 0.4674433469772339 0.9493203163146973\n",
      "Loss and prediction by the model after 4790000 steps of gradient descent are 0.4669109880924225 0.8631002902984619\n",
      "Loss and prediction by the model after 4800000 steps of gradient descent are 0.46632036566734314 0.9841790795326233\n",
      "Loss and prediction by the model after 4810000 steps of gradient descent are 0.46576789021492004 0.9588538408279419\n",
      "Loss and prediction by the model after 4820000 steps of gradient descent are 0.4652043282985687 0.9941584467887878\n",
      "Loss and prediction by the model after 4830000 steps of gradient descent are 0.4646625220775604 0.9520611763000488\n",
      "Loss and prediction by the model after 4840000 steps of gradient descent are 0.4641038179397583 0.9999995231628418\n",
      "Loss and prediction by the model after 4850000 steps of gradient descent are 0.4636031687259674 0.8154467344284058\n",
      "Loss and prediction by the model after 4860000 steps of gradient descent are 0.463001549243927 0.9885491132736206\n",
      "Loss and prediction by the model after 4870000 steps of gradient descent are 0.4624534845352173 0.9934170246124268\n",
      "Loss and prediction by the model after 4880000 steps of gradient descent are 0.4618932902812958 0.9957365989685059\n",
      "Loss and prediction by the model after 4890000 steps of gradient descent are 0.46134430170059204 0.994097113609314\n",
      "Loss and prediction by the model after 4900000 steps of gradient descent are 0.46079903841018677 0.9959398508071899\n",
      "Loss and prediction by the model after 4910000 steps of gradient descent are 0.46024608612060547 0.9534294009208679\n",
      "Loss and prediction by the model after 4920000 steps of gradient descent are 0.45968690514564514 0.9996101260185242\n",
      "Loss and prediction by the model after 4930000 steps of gradient descent are 0.4591388404369354 0.9997768998146057\n",
      "Loss and prediction by the model after 4940000 steps of gradient descent are 0.4586736559867859 0.7817373871803284\n",
      "Loss and prediction by the model after 4950000 steps of gradient descent are 0.4580898582935333 0.9429852366447449\n",
      "Loss and prediction by the model after 4960000 steps of gradient descent are 0.45751839876174927 0.9949117302894592\n",
      "Loss and prediction by the model after 4970000 steps of gradient descent are 0.4570462703704834 0.7782801389694214\n",
      "Loss and prediction by the model after 4980000 steps of gradient descent are 0.45710527896881104 0.3727700114250183\n",
      "Loss and prediction by the model after 4990000 steps of gradient descent are 0.45588505268096924 0.9685967564582825\n",
      "Loss and prediction by the model after 5000000 steps of gradient descent are 0.45533379912376404 0.984230637550354\n",
      "Loss and prediction by the model after 5010000 steps of gradient descent are 0.4547984004020691 0.9140400886535645\n",
      "Loss and prediction by the model after 5020000 steps of gradient descent are 0.4545300602912903 0.5285723209381104\n",
      "Loss and prediction by the model after 5030000 steps of gradient descent are 0.45382827520370483 0.6981147527694702\n",
      "Loss and prediction by the model after 5040000 steps of gradient descent are 0.45329830050468445 0.7110057473182678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss and prediction by the model after 5050000 steps of gradient descent are 0.45264068245887756 0.9889603853225708\n",
      "Loss and prediction by the model after 5060000 steps of gradient descent are 0.45217254757881165 0.7479708194732666\n",
      "Loss and prediction by the model after 5070000 steps of gradient descent are 0.452239066362381 0.37016505002975464\n",
      "Loss and prediction by the model after 5080000 steps of gradient descent are 0.4511036276817322 0.7071216106414795\n",
      "Loss and prediction by the model after 5090000 steps of gradient descent are 0.4504639804363251 0.8908933401107788\n",
      "Loss and prediction by the model after 5100000 steps of gradient descent are 0.44989514350891113 0.9715358018875122\n",
      "Loss and prediction by the model after 5110000 steps of gradient descent are 0.4493601322174072 0.9101072549819946\n",
      "Loss and prediction by the model after 5120000 steps of gradient descent are 0.44891369342803955 0.7034676671028137\n",
      "Loss and prediction by the model after 5130000 steps of gradient descent are 0.4482530951499939 0.9988390207290649\n",
      "Loss and prediction by the model after 5140000 steps of gradient descent are 0.4477359652519226 0.9426928162574768\n",
      "Loss and prediction by the model after 5150000 steps of gradient descent are 0.44718775153160095 0.9932995438575745\n",
      "Loss and prediction by the model after 5160000 steps of gradient descent are 0.4466474652290344 0.9958871603012085\n",
      "Loss and prediction by the model after 5170000 steps of gradient descent are 0.44612976908683777 0.9960135817527771\n",
      "Loss and prediction by the model after 5180000 steps of gradient descent are 0.4455740749835968 0.9996265172958374\n",
      "Loss and prediction by the model after 5190000 steps of gradient descent are 0.44502535462379456 0.9947758913040161\n",
      "Loss and prediction by the model after 5200000 steps of gradient descent are 0.4444945156574249 0.9968684315681458\n",
      "Loss and prediction by the model after 5210000 steps of gradient descent are 0.443980872631073 0.8998302817344666\n",
      "Loss and prediction by the model after 5220000 steps of gradient descent are 0.44345375895500183 0.8606275916099548\n",
      "Loss and prediction by the model after 5230000 steps of gradient descent are 0.4428730309009552 0.9528261423110962\n",
      "Loss and prediction by the model after 5240000 steps of gradient descent are 0.44232672452926636 0.9758581519126892\n",
      "Loss and prediction by the model after 5250000 steps of gradient descent are 0.44178277254104614 0.976345956325531\n",
      "Loss and prediction by the model after 5260000 steps of gradient descent are 0.4412337839603424 0.9982466101646423\n",
      "Loss and prediction by the model after 5270000 steps of gradient descent are 0.4406992197036743 0.9989643096923828\n",
      "Loss and prediction by the model after 5280000 steps of gradient descent are 0.44036462903022766 0.6084708571434021\n",
      "Loss and prediction by the model after 5290000 steps of gradient descent are 0.43974363803863525 0.7053713202476501\n",
      "Loss and prediction by the model after 5300000 steps of gradient descent are 0.4390859305858612 0.9970707893371582\n",
      "Loss and prediction by the model after 5310000 steps of gradient descent are 0.43856215476989746 0.9809375405311584\n",
      "Loss and prediction by the model after 5320000 steps of gradient descent are 0.4380229115486145 0.9963656663894653\n",
      "Loss and prediction by the model after 5330000 steps of gradient descent are 0.4374711215496063 0.9986099600791931\n",
      "Loss and prediction by the model after 5340000 steps of gradient descent are 0.4369320273399353 0.9976504445075989\n",
      "Loss and prediction by the model after 5350000 steps of gradient descent are 0.43642276525497437 0.9721850752830505\n",
      "Loss and prediction by the model after 5360000 steps of gradient descent are 0.4358796775341034 0.999160647392273\n",
      "Loss and prediction by the model after 5370000 steps of gradient descent are 0.4353519678115845 0.935073733329773\n",
      "Loss and prediction by the model after 5380000 steps of gradient descent are 0.4348407983779907 0.8600214719772339\n",
      "Loss and prediction by the model after 5390000 steps of gradient descent are 0.435210257768631 0.33002957701683044\n",
      "Loss and prediction by the model after 5400000 steps of gradient descent are 0.4337291121482849 0.9986595511436462\n",
      "Loss and prediction by the model after 5410000 steps of gradient descent are 0.4332432150840759 0.831532895565033\n",
      "Loss and prediction by the model after 5420000 steps of gradient descent are 0.43265753984451294 0.9699089527130127\n",
      "Loss and prediction by the model after 5430000 steps of gradient descent are 0.4328297972679138 0.37607553601264954\n",
      "Loss and prediction by the model after 5440000 steps of gradient descent are 0.4316039979457855 0.9982132911682129\n",
      "Loss and prediction by the model after 5450000 steps of gradient descent are 0.4310593605041504 0.9999984502792358\n",
      "Loss and prediction by the model after 5460000 steps of gradient descent are 0.4305279552936554 0.9964876174926758\n",
      "Loss and prediction by the model after 5470000 steps of gradient descent are 0.4300366938114166 0.8404394388198853\n",
      "Loss and prediction by the model after 5480000 steps of gradient descent are 0.42944565415382385 0.9923450946807861\n",
      "Loss and prediction by the model after 5490000 steps of gradient descent are 0.4289684295654297 0.802323043346405\n",
      "Loss and prediction by the model after 5500000 steps of gradient descent are 0.42840591073036194 0.9668291211128235\n",
      "Loss and prediction by the model after 5510000 steps of gradient descent are 0.42849811911582947 0.3957612216472626\n",
      "Loss and prediction by the model after 5520000 steps of gradient descent are 0.42734870314598083 0.9716519713401794\n",
      "Loss and prediction by the model after 5530000 steps of gradient descent are 0.4268084168434143 0.9989861845970154\n",
      "Loss and prediction by the model after 5540000 steps of gradient descent are 0.42630937695503235 0.8837599754333496\n",
      "Loss and prediction by the model after 5550000 steps of gradient descent are 0.4257434904575348 1.0\n",
      "Loss and prediction by the model after 5560000 steps of gradient descent are 0.42522984743118286 0.9001318216323853\n",
      "Loss and prediction by the model after 5570000 steps of gradient descent are 0.42469608783721924 0.9715858697891235\n",
      "Loss and prediction by the model after 5580000 steps of gradient descent are 0.42416617274284363 0.930484414100647\n",
      "Loss and prediction by the model after 5590000 steps of gradient descent are 0.42359864711761475 0.9999386668205261\n",
      "Loss and prediction by the model after 5600000 steps of gradient descent are 0.4230676293373108 0.9999747276306152\n",
      "Loss and prediction by the model after 5610000 steps of gradient descent are 0.4225209951400757 0.9999459385871887\n",
      "Loss and prediction by the model after 5620000 steps of gradient descent are 0.4219973385334015 0.9350603818893433\n",
      "Loss and prediction by the model after 5630000 steps of gradient descent are 0.4214445948600769 0.9680933356285095\n",
      "Loss and prediction by the model after 5640000 steps of gradient descent are 0.420906662940979 0.9999163150787354\n",
      "Loss and prediction by the model after 5650000 steps of gradient descent are 0.4203963875770569 0.9194564819335938\n",
      "Loss and prediction by the model after 5660000 steps of gradient descent are 0.4198535084724426 0.9157935976982117\n",
      "Loss and prediction by the model after 5670000 steps of gradient descent are 0.4193049371242523 0.9986741542816162\n",
      "Loss and prediction by the model after 5680000 steps of gradient descent are 0.4187842309474945 0.9873099327087402\n",
      "Loss and prediction by the model after 5690000 steps of gradient descent are 0.41824448108673096 0.9951409101486206\n",
      "Loss and prediction by the model after 5700000 steps of gradient descent are 0.4176998734474182 0.9809437990188599\n",
      "Loss and prediction by the model after 5710000 steps of gradient descent are 0.4171571135520935 0.9621642231941223\n",
      "Loss and prediction by the model after 5720000 steps of gradient descent are 0.4166482090950012 0.8909648656845093\n",
      "Loss and prediction by the model after 5730000 steps of gradient descent are 0.4162594974040985 0.6705219745635986\n",
      "Loss and prediction by the model after 5740000 steps of gradient descent are 0.41558751463890076 0.9258705973625183\n",
      "Loss and prediction by the model after 5750000 steps of gradient descent are 0.4150259494781494 0.9999182224273682\n",
      "Loss and prediction by the model after 5760000 steps of gradient descent are 0.4145430028438568 0.7885966300964355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss and prediction by the model after 5770000 steps of gradient descent are 0.4139348268508911 0.9971844553947449\n",
      "Loss and prediction by the model after 5780000 steps of gradient descent are 0.4133801758289337 0.994024395942688\n",
      "Loss and prediction by the model after 5790000 steps of gradient descent are 0.41285228729248047 0.9906386137008667\n",
      "Loss and prediction by the model after 5800000 steps of gradient descent are 0.4123106300830841 0.9998214840888977\n",
      "Loss and prediction by the model after 5810000 steps of gradient descent are 0.4119669497013092 0.6165210008621216\n",
      "Loss and prediction by the model after 5820000 steps of gradient descent are 0.41123008728027344 0.9868899583816528\n",
      "Loss and prediction by the model after 5830000 steps of gradient descent are 0.410675585269928 0.9911271929740906\n",
      "Loss and prediction by the model after 5840000 steps of gradient descent are 0.410160094499588 0.9274881482124329\n",
      "Loss and prediction by the model after 5850000 steps of gradient descent are 0.409639447927475 0.8670954704284668\n",
      "Loss and prediction by the model after 5860000 steps of gradient descent are 0.40998825430870056 0.34208598732948303\n",
      "Loss and prediction by the model after 5870000 steps of gradient descent are 0.40910103917121887 0.4260217547416687\n",
      "Loss and prediction by the model after 5880000 steps of gradient descent are 0.40802067518234253 0.9972484707832336\n",
      "Loss and prediction by the model after 5890000 steps of gradient descent are 0.40799447894096375 0.4397839307785034\n",
      "Loss and prediction by the model after 5900000 steps of gradient descent are 0.4069966971874237 0.8210570812225342\n",
      "Loss and prediction by the model after 5910000 steps of gradient descent are 0.406433641910553 0.8830059170722961\n",
      "Loss and prediction by the model after 5920000 steps of gradient descent are 0.4059310555458069 0.8159814476966858\n",
      "Loss and prediction by the model after 5930000 steps of gradient descent are 0.40532082319259644 0.986645519733429\n",
      "Loss and prediction by the model after 5940000 steps of gradient descent are 0.4047846496105194 0.9448161721229553\n",
      "Loss and prediction by the model after 5950000 steps of gradient descent are 0.40423426032066345 0.9986027479171753\n",
      "Loss and prediction by the model after 5960000 steps of gradient descent are 0.40367749333381653 0.9827059507369995\n",
      "Loss and prediction by the model after 5970000 steps of gradient descent are 0.4031636714935303 0.9371441602706909\n",
      "Loss and prediction by the model after 5980000 steps of gradient descent are 0.40262818336486816 0.9933005571365356\n",
      "Loss and prediction by the model after 5990000 steps of gradient descent are 0.4020833671092987 0.9932294487953186\n",
      "Loss and prediction by the model after 6000000 steps of gradient descent are 0.40158891677856445 0.8329005241394043\n",
      "Loss and prediction by the model after 6010000 steps of gradient descent are 0.4209880232810974 0.07774805277585983\n",
      "Loss and prediction by the model after 6020000 steps of gradient descent are 0.400437593460083 0.9857017993927002\n",
      "Loss and prediction by the model after 6030000 steps of gradient descent are 0.3999041020870209 0.95693039894104\n",
      "Loss and prediction by the model after 6040000 steps of gradient descent are 0.3993450701236725 0.9964407682418823\n",
      "Loss and prediction by the model after 6050000 steps of gradient descent are 0.3987855017185211 0.9999276399612427\n",
      "Loss and prediction by the model after 6060000 steps of gradient descent are 0.39839285612106323 0.6728090643882751\n",
      "Loss and prediction by the model after 6070000 steps of gradient descent are 0.564210832118988 0.024978825822472572\n",
      "Loss and prediction by the model after 6080000 steps of gradient descent are 0.3971982002258301 0.9131651520729065\n",
      "Loss and prediction by the model after 6090000 steps of gradient descent are 0.39669913053512573 0.7741315364837646\n",
      "Loss and prediction by the model after 6100000 steps of gradient descent are 0.396065354347229 0.9867461919784546\n",
      "Loss and prediction by the model after 6110000 steps of gradient descent are 0.3955533504486084 0.91585373878479\n",
      "Loss and prediction by the model after 6120000 steps of gradient descent are 0.3953821361064911 0.4917669892311096\n",
      "Loss and prediction by the model after 6130000 steps of gradient descent are 0.3953206241130829 0.3504379391670227\n",
      "Loss and prediction by the model after 6140000 steps of gradient descent are 0.39386096596717834 0.9984861612319946\n",
      "Loss and prediction by the model after 6150000 steps of gradient descent are 0.39341238141059875 0.7099727392196655\n",
      "Loss and prediction by the model after 6160000 steps of gradient descent are 0.3927658498287201 0.8507628440856934\n",
      "Loss and prediction by the model after 6170000 steps of gradient descent are 0.3921462297439575 0.9898765087127686\n",
      "Loss and prediction by the model after 6180000 steps of gradient descent are 0.3915952444076538 0.9278140664100647\n",
      "Loss and prediction by the model after 6190000 steps of gradient descent are 0.3909922242164612 0.9978309869766235\n",
      "Loss and prediction by the model after 6200000 steps of gradient descent are 0.39041659235954285 0.9975964426994324\n",
      "Loss and prediction by the model after 6210000 steps of gradient descent are 0.3898897171020508 0.962492823600769\n",
      "Loss and prediction by the model after 6220000 steps of gradient descent are 0.38933220505714417 0.9384908080101013\n",
      "Loss and prediction by the model after 6230000 steps of gradient descent are 0.3887442350387573 0.9999909400939941\n",
      "Loss and prediction by the model after 6240000 steps of gradient descent are 0.3881700336933136 0.992080569267273\n",
      "Loss and prediction by the model after 6250000 steps of gradient descent are 0.3876401484012604 0.9388120770454407\n",
      "Loss and prediction by the model after 6260000 steps of gradient descent are 0.3877001702785492 0.40961605310440063\n",
      "Loss and prediction by the model after 6270000 steps of gradient descent are 0.3865353465080261 0.8433557748794556\n",
      "Loss and prediction by the model after 6280000 steps of gradient descent are 0.3859289586544037 0.956446647644043\n",
      "Loss and prediction by the model after 6290000 steps of gradient descent are 0.3853834569454193 0.9447636008262634\n",
      "Loss and prediction by the model after 6300000 steps of gradient descent are 0.3848046064376831 0.9619669914245605\n",
      "Loss and prediction by the model after 6310000 steps of gradient descent are 0.38455426692962646 0.5337249040603638\n",
      "Loss and prediction by the model after 6320000 steps of gradient descent are 0.38368117809295654 0.9725648164749146\n",
      "Loss and prediction by the model after 6330000 steps of gradient descent are 0.3831164538860321 0.999879002571106\n",
      "Loss and prediction by the model after 6340000 steps of gradient descent are 0.3826082944869995 0.8166571855545044\n",
      "Loss and prediction by the model after 6350000 steps of gradient descent are 0.3819750249385834 0.9776126742362976\n",
      "Loss and prediction by the model after 6360000 steps of gradient descent are 0.3813878893852234 0.9914389252662659\n",
      "Loss and prediction by the model after 6370000 steps of gradient descent are 0.3810625374317169 0.5787305235862732\n",
      "Loss and prediction by the model after 6380000 steps of gradient descent are 0.3802310824394226 0.996373176574707\n",
      "Loss and prediction by the model after 6390000 steps of gradient descent are 0.37966227531433105 0.9970017075538635\n",
      "Loss and prediction by the model after 6400000 steps of gradient descent are 0.3791828453540802 0.7775864601135254\n",
      "Loss and prediction by the model after 6410000 steps of gradient descent are 0.3785363435745239 0.9825028777122498\n",
      "Loss and prediction by the model after 6420000 steps of gradient descent are 0.37796157598495483 0.9995378255844116\n",
      "Loss and prediction by the model after 6430000 steps of gradient descent are 0.37765568494796753 0.5870485305786133\n",
      "Loss and prediction by the model after 6440000 steps of gradient descent are 0.37686967849731445 0.9897799491882324\n",
      "Loss and prediction by the model after 6450000 steps of gradient descent are 0.3762975037097931 0.9980940222740173\n",
      "Loss and prediction by the model after 6460000 steps of gradient descent are 0.3757414221763611 0.9997171759605408\n",
      "Loss and prediction by the model after 6470000 steps of gradient descent are 0.3751760423183441 0.9999492764472961\n",
      "Loss and prediction by the model after 6480000 steps of gradient descent are 0.3755955994129181 0.3436584770679474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss and prediction by the model after 6490000 steps of gradient descent are 0.3741622865200043 0.7095600366592407\n",
      "Loss and prediction by the model after 6500000 steps of gradient descent are 0.3734702169895172 0.9684703350067139\n",
      "Loss and prediction by the model after 6510000 steps of gradient descent are 0.37287500500679016 0.9999052882194519\n",
      "Loss and prediction by the model after 6520000 steps of gradient descent are 0.3723025321960449 0.9979071617126465\n",
      "Loss and prediction by the model after 6530000 steps of gradient descent are 0.37176182866096497 0.8492884635925293\n",
      "Loss and prediction by the model after 6540000 steps of gradient descent are 0.37115615606307983 0.9986690878868103\n",
      "Loss and prediction by the model after 6550000 steps of gradient descent are 0.37059423327445984 0.9643450379371643\n",
      "Loss and prediction by the model after 6560000 steps of gradient descent are 0.3701602518558502 0.6762418150901794\n",
      "Loss and prediction by the model after 6570000 steps of gradient descent are 0.36944320797920227 0.9165149927139282\n",
      "Loss and prediction by the model after 6580000 steps of gradient descent are 0.3688514530658722 0.9245462417602539\n",
      "Loss and prediction by the model after 6590000 steps of gradient descent are 0.36834248900413513 0.7793300151824951\n",
      "Loss and prediction by the model after 6600000 steps of gradient descent are 0.3676854074001312 0.9980392456054688\n",
      "Loss and prediction by the model after 6610000 steps of gradient descent are 0.3671383261680603 0.9484362602233887\n",
      "Loss and prediction by the model after 6620000 steps of gradient descent are 0.36654332280158997 0.9999067187309265\n",
      "Loss and prediction by the model after 6630000 steps of gradient descent are 0.365959107875824 0.9988967180252075\n",
      "Loss and prediction by the model after 6640000 steps of gradient descent are 0.3655679225921631 0.6457974910736084\n",
      "Loss and prediction by the model after 6650000 steps of gradient descent are 0.3648039996623993 0.9899329543113708\n",
      "Loss and prediction by the model after 6660000 steps of gradient descent are 0.36512574553489685 0.3650669455528259\n",
      "Loss and prediction by the model after 6670000 steps of gradient descent are 0.3636533319950104 0.9995588660240173\n",
      "Loss and prediction by the model after 6680000 steps of gradient descent are 0.36306339502334595 0.9985295534133911\n",
      "Loss and prediction by the model after 6690000 steps of gradient descent are 0.3627239465713501 0.5869900584220886\n",
      "Loss and prediction by the model after 6700000 steps of gradient descent are 0.3618709444999695 0.9966955780982971\n",
      "Loss and prediction by the model after 6710000 steps of gradient descent are 0.3613475561141968 0.9437088370323181\n",
      "Loss and prediction by the model after 6720000 steps of gradient descent are 0.3607562184333801 0.9852909445762634\n",
      "Loss and prediction by the model after 6730000 steps of gradient descent are 0.36018797755241394 0.9537752270698547\n",
      "Loss and prediction by the model after 6740000 steps of gradient descent are 0.35957974195480347 0.9905277490615845\n",
      "Loss and prediction by the model after 6750000 steps of gradient descent are 0.3591391146183014 0.7135884165763855\n",
      "Loss and prediction by the model after 6760000 steps of gradient descent are 0.36140176653862 0.21082915365695953\n",
      "Loss and prediction by the model after 6770000 steps of gradient descent are 0.3583625257015228 0.46695318818092346\n",
      "Loss and prediction by the model after 6780000 steps of gradient descent are 0.3572779595851898 0.9692481160163879\n",
      "Loss and prediction by the model after 6790000 steps of gradient descent are 0.35666611790657043 0.9929196834564209\n",
      "Loss and prediction by the model after 6800000 steps of gradient descent are 0.35606318712234497 0.9871701598167419\n",
      "Loss and prediction by the model after 6810000 steps of gradient descent are 0.35548344254493713 0.9155263304710388\n",
      "Loss and prediction by the model after 6820000 steps of gradient descent are 0.3548494577407837 0.9982252717018127\n",
      "Loss and prediction by the model after 6830000 steps of gradient descent are 0.3542388677597046 0.9983770847320557\n",
      "Loss and prediction by the model after 6840000 steps of gradient descent are 0.3537324368953705 0.800344705581665\n",
      "Loss and prediction by the model after 6850000 steps of gradient descent are 0.3530578911304474 0.9999806880950928\n",
      "Loss and prediction by the model after 6860000 steps of gradient descent are 0.35249054431915283 0.9306923747062683\n",
      "Loss and prediction by the model after 6870000 steps of gradient descent are 0.35188573598861694 0.9481118321418762\n",
      "Loss and prediction by the model after 6880000 steps of gradient descent are 0.35127493739128113 0.9581450819969177\n",
      "Loss and prediction by the model after 6890000 steps of gradient descent are 0.3506588339805603 0.9986353516578674\n",
      "Loss and prediction by the model after 6900000 steps of gradient descent are 0.35009244084358215 0.9185145497322083\n",
      "Loss and prediction by the model after 6910000 steps of gradient descent are 0.34963032603263855 0.7300506830215454\n",
      "Loss and prediction by the model after 6920000 steps of gradient descent are 0.49079081416130066 0.028959298506379128\n",
      "Loss and prediction by the model after 6930000 steps of gradient descent are 0.3487963080406189 0.4782916307449341\n",
      "Loss and prediction by the model after 6940000 steps of gradient descent are 0.34771353006362915 0.9939633011817932\n",
      "Loss and prediction by the model after 6950000 steps of gradient descent are 0.347120076417923 0.9989424347877502\n",
      "Loss and prediction by the model after 6960000 steps of gradient descent are 0.34653133153915405 0.9799618721008301\n",
      "Loss and prediction by the model after 6970000 steps of gradient descent are 0.34595945477485657 0.927139401435852\n",
      "Loss and prediction by the model after 6980000 steps of gradient descent are 0.3453521430492401 0.9658812284469604\n",
      "Loss and prediction by the model after 6990000 steps of gradient descent are 0.3447742760181427 0.9639469385147095\n",
      "Loss and prediction by the model after 7000000 steps of gradient descent are 0.34424757957458496 0.8569673895835876\n",
      "Loss and prediction by the model after 7010000 steps of gradient descent are 0.3436858654022217 0.8407970666885376\n",
      "Loss and prediction by the model after 7020000 steps of gradient descent are 0.343683660030365 0.43507418036460876\n",
      "Loss and prediction by the model after 7030000 steps of gradient descent are 0.34248262643814087 0.9991838335990906\n",
      "Loss and prediction by the model after 7040000 steps of gradient descent are 0.3419046700000763 0.9986869096755981\n",
      "Loss and prediction by the model after 7050000 steps of gradient descent are 0.3418239653110504 0.48159852623939514\n",
      "Loss and prediction by the model after 7060000 steps of gradient descent are 0.34078216552734375 0.9854630827903748\n",
      "Loss and prediction by the model after 7070000 steps of gradient descent are 0.3402060568332672 0.9343401789665222\n",
      "Loss and prediction by the model after 7080000 steps of gradient descent are 0.3395971655845642 0.9895948171615601\n",
      "Loss and prediction by the model after 7090000 steps of gradient descent are 0.3392593264579773 0.6099213361740112\n",
      "Loss and prediction by the model after 7100000 steps of gradient descent are 0.33850666880607605 0.8700923919677734\n",
      "Loss and prediction by the model after 7110000 steps of gradient descent are 0.33787456154823303 0.9772308468818665\n",
      "Loss and prediction by the model after 7120000 steps of gradient descent are 0.3373645544052124 0.8061853647232056\n",
      "Loss and prediction by the model after 7130000 steps of gradient descent are 0.33669599890708923 0.9987682700157166\n",
      "Loss and prediction by the model after 7140000 steps of gradient descent are 0.33613893389701843 0.8891358375549316\n",
      "Loss and prediction by the model after 7150000 steps of gradient descent are 0.33551543951034546 0.9974498748779297\n",
      "Loss and prediction by the model after 7160000 steps of gradient descent are 0.4609198570251465 0.03156502544879913\n",
      "Loss and prediction by the model after 7170000 steps of gradient descent are 0.33503079414367676 0.4226469397544861\n",
      "Loss and prediction by the model after 7180000 steps of gradient descent are 0.33378660678863525 0.9073948264122009\n",
      "Loss and prediction by the model after 7190000 steps of gradient descent are 0.3334197700023651 0.6071457266807556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss and prediction by the model after 7200000 steps of gradient descent are 0.3327881693840027 0.6506374478340149\n",
      "Loss and prediction by the model after 7210000 steps of gradient descent are 0.3319994807243347 0.9773258566856384\n",
      "Loss and prediction by the model after 7220000 steps of gradient descent are 0.3331470787525177 0.28371819853782654\n",
      "Loss and prediction by the model after 7230000 steps of gradient descent are 0.33090442419052124 0.8499352335929871\n",
      "Loss and prediction by the model after 7240000 steps of gradient descent are 0.3302901089191437 0.9379998445510864\n",
      "Loss and prediction by the model after 7250000 steps of gradient descent are 0.32971152663230896 0.9531608819961548\n",
      "Loss and prediction by the model after 7260000 steps of gradient descent are 0.329127699136734 0.9802190065383911\n",
      "Loss and prediction by the model after 7270000 steps of gradient descent are 0.3285800516605377 0.8864484429359436\n",
      "Loss and prediction by the model after 7280000 steps of gradient descent are 0.32796749472618103 0.9861178398132324\n",
      "Loss and prediction by the model after 7290000 steps of gradient descent are 0.32740554213523865 0.9711605310440063\n",
      "Loss and prediction by the model after 7300000 steps of gradient descent are 0.32685038447380066 0.9553408622741699\n",
      "Loss and prediction by the model after 7310000 steps of gradient descent are 0.3263790011405945 0.7575422525405884\n",
      "Loss and prediction by the model after 7320000 steps of gradient descent are 0.3257105052471161 0.9982122778892517\n",
      "Loss and prediction by the model after 7330000 steps of gradient descent are 0.32529398798942566 0.7069979310035706\n",
      "Loss and prediction by the model after 7340000 steps of gradient descent are 0.3246155381202698 0.8861825466156006\n",
      "Loss and prediction by the model after 7350000 steps of gradient descent are 0.5453552603721619 0.022790154442191124\n",
      "Loss and prediction by the model after 7360000 steps of gradient descent are 0.3234785795211792 0.8909574747085571\n",
      "Loss and prediction by the model after 7370000 steps of gradient descent are 0.3228633999824524 0.987065851688385\n",
      "Loss and prediction by the model after 7380000 steps of gradient descent are 0.32232773303985596 0.8902205228805542\n",
      "Loss and prediction by the model after 7390000 steps of gradient descent are 0.32190755009651184 0.6623227596282959\n",
      "Loss and prediction by the model after 7400000 steps of gradient descent are 0.32116249203681946 0.9235339164733887\n",
      "Loss and prediction by the model after 7410000 steps of gradient descent are 0.32057562470436096 0.9764843583106995\n",
      "Loss and prediction by the model after 7420000 steps of gradient descent are 0.3200089931488037 0.9956745505332947\n",
      "Loss and prediction by the model after 7430000 steps of gradient descent are 0.3194698095321655 0.9466923475265503\n",
      "Loss and prediction by the model after 7440000 steps of gradient descent are 0.3189457654953003 0.8328763842582703\n",
      "Loss and prediction by the model after 7450000 steps of gradient descent are 0.31853944063186646 0.6545014381408691\n",
      "Loss and prediction by the model after 7460000 steps of gradient descent are 0.3177679479122162 0.9976909160614014\n",
      "Loss and prediction by the model after 7470000 steps of gradient descent are 0.3172524571418762 0.8872714638710022\n",
      "Loss and prediction by the model after 7480000 steps of gradient descent are 0.316666841506958 0.9824954271316528\n",
      "Loss and prediction by the model after 7490000 steps of gradient descent are 0.3161255717277527 0.9999451637268066\n",
      "Loss and prediction by the model after 7500000 steps of gradient descent are 0.3155699670314789 0.9960240721702576\n",
      "Loss and prediction by the model after 7510000 steps of gradient descent are 0.3150101900100708 0.9982691407203674\n",
      "Loss and prediction by the model after 7520000 steps of gradient descent are 0.3553928732872009 0.060280054807662964\n",
      "Loss and prediction by the model after 7530000 steps of gradient descent are 0.31390267610549927 0.9938458800315857\n",
      "Loss and prediction by the model after 7540000 steps of gradient descent are 0.3133753538131714 0.8964405059814453\n",
      "Loss and prediction by the model after 7550000 steps of gradient descent are 0.3131844997406006 0.5383027195930481\n",
      "Loss and prediction by the model after 7560000 steps of gradient descent are 0.31224340200424194 0.9997127056121826\n",
      "Loss and prediction by the model after 7570000 steps of gradient descent are 0.3116987943649292 0.9890079498291016\n",
      "Loss and prediction by the model after 7580000 steps of gradient descent are 0.31112319231033325 0.9980273246765137\n",
      "Loss and prediction by the model after 7590000 steps of gradient descent are 0.3113839328289032 0.4084801971912384\n",
      "Loss and prediction by the model after 7600000 steps of gradient descent are 0.3104095458984375 0.5393572449684143\n",
      "Loss and prediction by the model after 7610000 steps of gradient descent are 0.3095383942127228 0.8610773086547852\n",
      "Loss and prediction by the model after 7620000 steps of gradient descent are 0.3089197278022766 0.9985385537147522\n",
      "Loss and prediction by the model after 7630000 steps of gradient descent are 0.5467172861099243 0.022145502269268036\n",
      "Loss and prediction by the model after 7640000 steps of gradient descent are 0.3078403174877167 0.936905562877655\n",
      "Loss and prediction by the model after 7650000 steps of gradient descent are 0.30739155411720276 0.7629233002662659\n",
      "Loss and prediction by the model after 7660000 steps of gradient descent are 0.30675479769706726 0.9946322441101074\n",
      "Loss and prediction by the model after 7670000 steps of gradient descent are 0.30621138215065 0.9983997344970703\n",
      "Loss and prediction by the model after 7680000 steps of gradient descent are 0.3056975305080414 0.9223393797874451\n",
      "Loss and prediction by the model after 7690000 steps of gradient descent are 0.3052111566066742 0.7964293360710144\n",
      "Loss and prediction by the model after 7700000 steps of gradient descent are 0.3045579195022583 0.9995648264884949\n",
      "Loss and prediction by the model after 7710000 steps of gradient descent are 0.30545634031295776 0.3194078207015991\n",
      "Loss and prediction by the model after 7720000 steps of gradient descent are 0.3034648299217224 0.9961842894554138\n",
      "Loss and prediction by the model after 7730000 steps of gradient descent are 0.30293008685112 0.9963988065719604\n",
      "Loss and prediction by the model after 7740000 steps of gradient descent are 0.3024037778377533 0.9915153384208679\n",
      "Loss and prediction by the model after 7750000 steps of gradient descent are 0.3018539547920227 0.9996718764305115\n",
      "Loss and prediction by the model after 7760000 steps of gradient descent are 0.3013405203819275 0.9407855868339539\n",
      "Loss and prediction by the model after 7770000 steps of gradient descent are 0.30081239342689514 0.9758809208869934\n",
      "Loss and prediction by the model after 7780000 steps of gradient descent are 0.3009479343891144 0.4447297751903534\n",
      "Loss and prediction by the model after 7790000 steps of gradient descent are 0.29973843693733215 0.9932973384857178\n",
      "Loss and prediction by the model after 7800000 steps of gradient descent are 0.3040221035480499 0.18222935497760773\n",
      "Loss and prediction by the model after 7810000 steps of gradient descent are 0.2990747392177582 0.5344899296760559\n",
      "Loss and prediction by the model after 7820000 steps of gradient descent are 0.298175573348999 0.8370144963264465\n",
      "Loss and prediction by the model after 7830000 steps of gradient descent are 0.29756349325180054 0.9931783676147461\n",
      "Loss and prediction by the model after 7840000 steps of gradient descent are 0.29703447222709656 0.99017333984375\n",
      "Loss and prediction by the model after 7850000 steps of gradient descent are 0.3011416792869568 0.18641294538974762\n",
      "Loss and prediction by the model after 7860000 steps of gradient descent are 0.29610392451286316 0.737920880317688\n",
      "Loss and prediction by the model after 7870000 steps of gradient descent are 0.2964437007904053 0.37623679637908936\n",
      "Loss and prediction by the model after 7880000 steps of gradient descent are 0.2949559986591339 0.9036475419998169\n",
      "Loss and prediction by the model after 7890000 steps of gradient descent are 0.29445672035217285 0.8205809593200684\n",
      "Loss and prediction by the model after 7900000 steps of gradient descent are 0.2938567101955414 0.9881398677825928\n",
      "Loss and prediction by the model after 7910000 steps of gradient descent are 0.29356637597084045 0.6374443173408508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss and prediction by the model after 7920000 steps of gradient descent are 0.3024730682373047 0.1304311454296112\n",
      "Loss and prediction by the model after 7930000 steps of gradient descent are 0.2922426164150238 0.9559184312820435\n",
      "Loss and prediction by the model after 7940000 steps of gradient descent are 0.29223012924194336 0.4903159737586975\n",
      "Loss and prediction by the model after 7950000 steps of gradient descent are 0.2913396954536438 0.7004358172416687\n",
      "Loss and prediction by the model after 7960000 steps of gradient descent are 0.29062357544898987 0.9763832688331604\n",
      "Loss and prediction by the model after 7970000 steps of gradient descent are 0.2901464104652405 0.8550482988357544\n",
      "Loss and prediction by the model after 7980000 steps of gradient descent are 0.28975552320480347 0.6646131277084351\n",
      "Loss and prediction by the model after 7990000 steps of gradient descent are 0.28906774520874023 0.8501378297805786\n",
      "Loss and prediction by the model after 8000000 steps of gradient descent are 0.28858038783073425 0.7916589379310608\n",
      "Loss and prediction by the model after 8010000 steps of gradient descent are 0.2879544794559479 0.9998776912689209\n",
      "Loss and prediction by the model after 8020000 steps of gradient descent are 0.2874208092689514 0.9739619493484497\n",
      "Loss and prediction by the model after 8030000 steps of gradient descent are 0.3097720146179199 0.08528507500886917\n",
      "Loss and prediction by the model after 8040000 steps of gradient descent are 0.286630243062973 0.6404434442520142\n",
      "Loss and prediction by the model after 8050000 steps of gradient descent are 0.28623783588409424 0.5664766430854797\n",
      "Loss and prediction by the model after 8060000 steps of gradient descent are 0.28536027669906616 0.9695320129394531\n",
      "Loss and prediction by the model after 8070000 steps of gradient descent are 0.28489261865615845 0.8354728817939758\n",
      "Loss and prediction by the model after 8080000 steps of gradient descent are 0.28431305289268494 0.9997894167900085\n",
      "Loss and prediction by the model after 8090000 steps of gradient descent are 0.2837992012500763 0.9977967739105225\n",
      "Loss and prediction by the model after 8100000 steps of gradient descent are 0.2833602726459503 0.8028692603111267\n",
      "Loss and prediction by the model after 8110000 steps of gradient descent are 0.28421682119369507 0.3280775249004364\n",
      "Loss and prediction by the model after 8120000 steps of gradient descent are 0.2822611629962921 0.970782458782196\n",
      "Loss and prediction by the model after 8130000 steps of gradient descent are 0.2817346751689911 0.9857434034347534\n",
      "Loss and prediction by the model after 8140000 steps of gradient descent are 0.28224968910217285 0.3823525905609131\n",
      "Loss and prediction by the model after 8150000 steps of gradient descent are 0.28074657917022705 0.8873738050460815\n",
      "Loss and prediction by the model after 8160000 steps of gradient descent are 0.28018996119499207 0.9544889330863953\n",
      "Loss and prediction by the model after 8170000 steps of gradient descent are 0.27964523434638977 0.9946011900901794\n",
      "Loss and prediction by the model after 8180000 steps of gradient descent are 0.2791252136230469 0.9877923130989075\n",
      "Loss and prediction by the model after 8190000 steps of gradient descent are 0.2802816927433014 0.3100999593734741\n",
      "Loss and prediction by the model after 8200000 steps of gradient descent are 0.2783607542514801 0.623241126537323\n",
      "Loss and prediction by the model after 8210000 steps of gradient descent are 0.27809199690818787 0.49195289611816406\n",
      "Loss and prediction by the model after 8220000 steps of gradient descent are 0.2770264744758606 0.9242820143699646\n",
      "Loss and prediction by the model after 8230000 steps of gradient descent are 0.27692607045173645 0.5427003502845764\n",
      "Loss and prediction by the model after 8240000 steps of gradient descent are 0.27599936723709106 0.9665319323539734\n",
      "Loss and prediction by the model after 8250000 steps of gradient descent are 0.2755434811115265 0.8348244428634644\n",
      "Loss and prediction by the model after 8260000 steps of gradient descent are 0.2749587297439575 0.9316676259040833\n",
      "Loss and prediction by the model after 8270000 steps of gradient descent are 0.27443209290504456 0.9858070611953735\n",
      "Loss and prediction by the model after 8280000 steps of gradient descent are 0.27389848232269287 0.9967770576477051\n",
      "Loss and prediction by the model after 8290000 steps of gradient descent are 0.27341070771217346 0.9916713833808899\n",
      "Loss and prediction by the model after 8300000 steps of gradient descent are 0.27337509393692017 0.5210431218147278\n",
      "Loss and prediction by the model after 8310000 steps of gradient descent are 0.27249792218208313 0.8149541020393372\n",
      "Loss and prediction by the model after 8320000 steps of gradient descent are 0.27230945229530334 0.5566752552986145\n",
      "Loss and prediction by the model after 8330000 steps of gradient descent are 0.27138611674308777 0.9994451403617859\n",
      "Loss and prediction by the model after 8340000 steps of gradient descent are 0.2712540328502655 0.576675534248352\n",
      "Loss and prediction by the model after 8350000 steps of gradient descent are 0.27058908343315125 0.6825559735298157\n",
      "Loss and prediction by the model after 8360000 steps of gradient descent are 0.2710114121437073 0.3741065263748169\n",
      "Loss and prediction by the model after 8370000 steps of gradient descent are 0.2695898711681366 0.6630151271820068\n",
      "Loss and prediction by the model after 8380000 steps of gradient descent are 0.2688664197921753 0.95778888463974\n",
      "Loss and prediction by the model after 8390000 steps of gradient descent are 0.26838988065719604 0.885376513004303\n",
      "Loss and prediction by the model after 8400000 steps of gradient descent are 0.2680739760398865 0.6592954397201538\n",
      "Loss and prediction by the model after 8410000 steps of gradient descent are 0.26812949776649475 0.439748078584671\n",
      "Loss and prediction by the model after 8420000 steps of gradient descent are 0.2671358287334442 0.6199453473091125\n",
      "Loss and prediction by the model after 8430000 steps of gradient descent are 0.28816255927085876 0.09048344194889069\n",
      "Loss and prediction by the model after 8440000 steps of gradient descent are 0.2658812701702118 0.8820791840553284\n",
      "Loss and prediction by the model after 8450000 steps of gradient descent are 0.2653166949748993 0.9486221075057983\n",
      "Loss and prediction by the model after 8460000 steps of gradient descent are 0.26482999324798584 0.8905232548713684\n",
      "Loss and prediction by the model after 8470000 steps of gradient descent are 0.26428964734077454 0.97504061460495\n",
      "Loss and prediction by the model after 8480000 steps of gradient descent are 0.26377880573272705 0.9911563992500305\n",
      "Loss and prediction by the model after 8490000 steps of gradient descent are 0.3367922306060791 0.04755496606230736\n",
      "Loss and prediction by the model after 8500000 steps of gradient descent are 0.26303279399871826 0.6553761959075928\n",
      "Loss and prediction by the model after 8510000 steps of gradient descent are 0.263126939535141 0.43177473545074463\n",
      "Loss and prediction by the model after 8520000 steps of gradient descent are 0.26179105043411255 0.9988923668861389\n",
      "Loss and prediction by the model after 8530000 steps of gradient descent are 0.2613028883934021 0.9555783867835999\n",
      "Loss and prediction by the model after 8540000 steps of gradient descent are 0.2608032822608948 0.9985783100128174\n",
      "Loss and prediction by the model after 8550000 steps of gradient descent are 0.2602817714214325 0.9983141422271729\n",
      "Loss and prediction by the model after 8560000 steps of gradient descent are 0.3646028935909271 0.03905750811100006\n",
      "Loss and prediction by the model after 8570000 steps of gradient descent are 0.2592865824699402 0.9638530611991882\n",
      "Loss and prediction by the model after 8580000 steps of gradient descent are 0.25876450538635254 0.9966124892234802\n",
      "Loss and prediction by the model after 8590000 steps of gradient descent are 0.25835028290748596 0.8297537565231323\n",
      "Loss and prediction by the model after 8600000 steps of gradient descent are 0.2581278681755066 0.5952441096305847\n",
      "Loss and prediction by the model after 8610000 steps of gradient descent are 0.25726842880249023 0.9951813817024231\n",
      "Loss and prediction by the model after 8620000 steps of gradient descent are 0.2567829191684723 0.9872211813926697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss and prediction by the model after 8630000 steps of gradient descent are 0.25645244121551514 0.747106671333313\n",
      "Loss and prediction by the model after 8640000 steps of gradient descent are 0.2563815116882324 0.5073428750038147\n",
      "Loss and prediction by the model after 8650000 steps of gradient descent are 0.2567630112171173 0.34455037117004395\n",
      "Loss and prediction by the model after 8660000 steps of gradient descent are 0.2548738420009613 0.9087705016136169\n",
      "Loss and prediction by the model after 8670000 steps of gradient descent are 0.2559967041015625 0.32494136691093445\n",
      "Loss and prediction by the model after 8680000 steps of gradient descent are 0.25464892387390137 0.44583094120025635\n",
      "Loss and prediction by the model after 8690000 steps of gradient descent are 0.25337135791778564 0.9947347044944763\n",
      "Loss and prediction by the model after 8700000 steps of gradient descent are 0.2542581856250763 0.3549342155456543\n",
      "Loss and prediction by the model after 8710000 steps of gradient descent are 0.25260427594184875 0.7133562564849854\n",
      "Loss and prediction by the model after 8720000 steps of gradient descent are 0.25210505723953247 0.7628495097160339\n",
      "Loss and prediction by the model after 8730000 steps of gradient descent are 0.25150808691978455 0.8879626989364624\n",
      "Loss and prediction by the model after 8740000 steps of gradient descent are 0.25099068880081177 0.9351595044136047\n",
      "Loss and prediction by the model after 8750000 steps of gradient descent are 0.2505510449409485 0.8228815793991089\n",
      "Loss and prediction by the model after 8760000 steps of gradient descent are 0.25022074580192566 0.6716013550758362\n",
      "Loss and prediction by the model after 8770000 steps of gradient descent are 0.2494647055864334 0.9993613362312317\n",
      "Loss and prediction by the model after 8780000 steps of gradient descent are 0.24917665123939514 0.7174204587936401\n",
      "Loss and prediction by the model after 8790000 steps of gradient descent are 0.2485441416501999 0.9779211282730103\n",
      "Loss and prediction by the model after 8800000 steps of gradient descent are 0.2490578293800354 0.4095049500465393\n",
      "Loss and prediction by the model after 8810000 steps of gradient descent are 0.2475958615541458 0.9998947978019714\n",
      "Loss and prediction by the model after 8820000 steps of gradient descent are 0.24710232019424438 0.9999584555625916\n",
      "Loss and prediction by the model after 8830000 steps of gradient descent are 0.24675214290618896 0.7922323942184448\n",
      "Loss and prediction by the model after 8840000 steps of gradient descent are 0.24723811447620392 0.39603757858276367\n",
      "Loss and prediction by the model after 8850000 steps of gradient descent are 0.24597793817520142 0.616834282875061\n",
      "Loss and prediction by the model after 8860000 steps of gradient descent are 0.2452339380979538 0.895889401435852\n",
      "Loss and prediction by the model after 8870000 steps of gradient descent are 0.24532322585582733 0.5021147131919861\n",
      "Loss and prediction by the model after 8880000 steps of gradient descent are 0.24430213868618011 0.9811241626739502\n",
      "Loss and prediction by the model after 8890000 steps of gradient descent are 0.24391520023345947 0.8507069945335388\n",
      "Loss and prediction by the model after 8900000 steps of gradient descent are 0.24336189031600952 0.9939103722572327\n",
      "Loss and prediction by the model after 8910000 steps of gradient descent are 0.24659428000450134 0.2291724979877472\n",
      "Loss and prediction by the model after 8920000 steps of gradient descent are 0.2441999912261963 0.3228066861629486\n",
      "Loss and prediction by the model after 8930000 steps of gradient descent are 0.24234504997730255 0.6163305044174194\n",
      "Loss and prediction by the model after 8940000 steps of gradient descent are 0.2416449338197708 0.9198031425476074\n",
      "Loss and prediction by the model after 8950000 steps of gradient descent are 0.24126118421554565 0.7847169041633606\n",
      "Loss and prediction by the model after 8960000 steps of gradient descent are 0.2407195121049881 0.8933200240135193\n",
      "Loss and prediction by the model after 8970000 steps of gradient descent are 0.24030356109142303 0.9701014757156372\n",
      "Loss and prediction by the model after 8980000 steps of gradient descent are 0.24113401770591736 0.3692685067653656\n",
      "Loss and prediction by the model after 8990000 steps of gradient descent are 0.24535372853279114 0.18204280734062195\n",
      "Loss and prediction by the model after 9000000 steps of gradient descent are 0.2421647608280182 0.24490085244178772\n",
      "Loss and prediction by the model after 9010000 steps of gradient descent are 0.2384987324476242 0.9881966710090637\n",
      "Loss and prediction by the model after 9020000 steps of gradient descent are 0.2380814403295517 0.9311819076538086\n",
      "Loss and prediction by the model after 9030000 steps of gradient descent are 0.2381727397441864 0.5184478163719177\n",
      "Loss and prediction by the model after 9040000 steps of gradient descent are 0.23717784881591797 0.9762886166572571\n",
      "Loss and prediction by the model after 9050000 steps of gradient descent are 0.23790454864501953 0.3849928081035614\n",
      "Loss and prediction by the model after 9060000 steps of gradient descent are 0.23666135966777802 0.5794603228569031\n",
      "Loss and prediction by the model after 9070000 steps of gradient descent are 0.2357717901468277 0.9998835325241089\n",
      "Loss and prediction by the model after 9080000 steps of gradient descent are 0.23530453443527222 0.9935091137886047\n",
      "Loss and prediction by the model after 9090000 steps of gradient descent are 0.23494838178157806 0.8229271769523621\n",
      "Loss and prediction by the model after 9100000 steps of gradient descent are 0.23458126187324524 0.7538819313049316\n",
      "Loss and prediction by the model after 9110000 steps of gradient descent are 0.23406150937080383 0.8478229641914368\n",
      "Loss and prediction by the model after 9120000 steps of gradient descent are 0.2335033416748047 0.9863046407699585\n",
      "Loss and prediction by the model after 9130000 steps of gradient descent are 0.23312599956989288 0.9033803343772888\n",
      "Loss and prediction by the model after 9140000 steps of gradient descent are 0.23276174068450928 0.7972968816757202\n",
      "Loss and prediction by the model after 9150000 steps of gradient descent are 0.232261523604393 0.8344879746437073\n",
      "Loss and prediction by the model after 9160000 steps of gradient descent are 0.2317485809326172 0.9828378558158875\n",
      "Loss and prediction by the model after 9170000 steps of gradient descent are 0.23134885728359222 0.9906182289123535\n",
      "Loss and prediction by the model after 9180000 steps of gradient descent are 0.2353130578994751 0.21469363570213318\n",
      "Loss and prediction by the model after 9190000 steps of gradient descent are 0.23060500621795654 0.7686421871185303\n",
      "Loss and prediction by the model after 9200000 steps of gradient descent are 0.23003248870372772 0.9967807531356812\n",
      "Loss and prediction by the model after 9210000 steps of gradient descent are 0.2295784056186676 0.9761589765548706\n",
      "Loss and prediction by the model after 9220000 steps of gradient descent are 0.23075413703918457 0.34321409463882446\n",
      "Loss and prediction by the model after 9230000 steps of gradient descent are 0.22929169237613678 0.5081550478935242\n",
      "Loss and prediction by the model after 9240000 steps of gradient descent are 0.22825033962726593 0.9212081432342529\n",
      "Loss and prediction by the model after 9250000 steps of gradient descent are 0.22777369618415833 0.9532168507575989\n",
      "Loss and prediction by the model after 9260000 steps of gradient descent are 0.2273198515176773 0.9991900324821472\n",
      "Loss and prediction by the model after 9270000 steps of gradient descent are 0.23961122334003448 0.12855274975299835\n",
      "Loss and prediction by the model after 9280000 steps of gradient descent are 0.2271946221590042 0.4714280664920807\n",
      "Loss and prediction by the model after 9290000 steps of gradient descent are 0.22809024155139923 0.3066016435623169\n",
      "Loss and prediction by the model after 9300000 steps of gradient descent are 0.22570523619651794 0.7484521865844727\n",
      "Loss and prediction by the model after 9310000 steps of gradient descent are 0.22510550916194916 0.9996770024299622\n",
      "Loss and prediction by the model after 9320000 steps of gradient descent are 0.2252143770456314 0.5437318682670593\n",
      "Loss and prediction by the model after 9330000 steps of gradient descent are 0.22424782812595367 0.9872231483459473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss and prediction by the model after 9340000 steps of gradient descent are 0.22414062917232513 0.6307467222213745\n",
      "Loss and prediction by the model after 9350000 steps of gradient descent are 0.2235068827867508 0.8802911639213562\n",
      "Loss and prediction by the model after 9360000 steps of gradient descent are 0.22306078672409058 0.906562089920044\n",
      "Loss and prediction by the model after 9370000 steps of gradient descent are 0.22274810075759888 0.7589889764785767\n",
      "Loss and prediction by the model after 9380000 steps of gradient descent are 0.22238853573799133 0.6976187825202942\n",
      "Loss and prediction by the model after 9390000 steps of gradient descent are 0.22194501757621765 0.6901302933692932\n",
      "Loss and prediction by the model after 9400000 steps of gradient descent are 0.22129328548908234 0.9566895365715027\n",
      "Loss and prediction by the model after 9410000 steps of gradient descent are 0.22090454399585724 0.9999610185623169\n",
      "Loss and prediction by the model after 9420000 steps of gradient descent are 0.22048364579677582 0.9987115859985352\n",
      "Loss and prediction by the model after 9430000 steps of gradient descent are 0.2210714966058731 0.4246029853820801\n",
      "Loss and prediction by the model after 9440000 steps of gradient descent are 0.21959955990314484 0.9928351044654846\n",
      "Loss and prediction by the model after 9450000 steps of gradient descent are 0.21926364302635193 0.9955211281776428\n",
      "Loss and prediction by the model after 9460000 steps of gradient descent are 0.21895486116409302 0.7748306393623352\n",
      "Loss and prediction by the model after 9470000 steps of gradient descent are 0.2193787693977356 0.4277418553829193\n",
      "Loss and prediction by the model after 9480000 steps of gradient descent are 0.21869529783725739 0.47778454422950745\n",
      "Loss and prediction by the model after 9490000 steps of gradient descent are 0.21833005547523499 0.4695224165916443\n",
      "Loss and prediction by the model after 9500000 steps of gradient descent are 0.21711987257003784 0.9924431443214417\n",
      "Loss and prediction by the model after 9510000 steps of gradient descent are 0.21681086719036102 0.8411288857460022\n",
      "Loss and prediction by the model after 9520000 steps of gradient descent are 0.21689675748348236 0.5904309749603271\n",
      "Loss and prediction by the model after 9530000 steps of gradient descent are 0.21606208384037018 0.9944261908531189\n",
      "Loss and prediction by the model after 9540000 steps of gradient descent are 0.2156347781419754 0.9849891066551208\n",
      "Loss and prediction by the model after 9550000 steps of gradient descent are 0.21556435525417328 0.6581020951271057\n",
      "Loss and prediction by the model after 9560000 steps of gradient descent are 0.30370599031448364 0.0465393140912056\n",
      "Loss and prediction by the model after 9570000 steps of gradient descent are 0.2144629806280136 0.9701292514801025\n",
      "Loss and prediction by the model after 9580000 steps of gradient descent are 0.21407505869865417 0.9017897248268127\n",
      "Loss and prediction by the model after 9590000 steps of gradient descent are 0.21362057328224182 0.9962309002876282\n",
      "Loss and prediction by the model after 9600000 steps of gradient descent are 0.21332065761089325 0.8402020931243896\n",
      "Loss and prediction by the model after 9610000 steps of gradient descent are 0.21459302306175232 0.3397197723388672\n",
      "Loss and prediction by the model after 9620000 steps of gradient descent are 0.21237345039844513 0.9979323744773865\n",
      "Loss and prediction by the model after 9630000 steps of gradient descent are 0.21219009160995483 0.6953350901603699\n",
      "Loss and prediction by the model after 9640000 steps of gradient descent are 0.2116401195526123 0.8288586735725403\n",
      "Loss and prediction by the model after 9650000 steps of gradient descent are 0.21116667985916138 0.8473241329193115\n",
      "Loss and prediction by the model after 9660000 steps of gradient descent are 0.21256661415100098 0.33204641938209534\n",
      "Loss and prediction by the model after 9670000 steps of gradient descent are 0.2106308788061142 0.6233298182487488\n",
      "Loss and prediction by the model after 9680000 steps of gradient descent are 0.20984384417533875 0.9945859313011169\n",
      "Loss and prediction by the model after 9690000 steps of gradient descent are 0.20947109162807465 0.9726257920265198\n",
      "Loss and prediction by the model after 9700000 steps of gradient descent are 0.20977230370044708 0.49961864948272705\n",
      "Loss and prediction by the model after 9710000 steps of gradient descent are 0.2087579071521759 0.887971818447113\n",
      "Loss and prediction by the model after 9720000 steps of gradient descent are 0.20846903324127197 0.7485980987548828\n",
      "Loss and prediction by the model after 9730000 steps of gradient descent are 0.20791761577129364 0.9760730266571045\n",
      "Loss and prediction by the model after 9740000 steps of gradient descent are 0.2338535189628601 0.0924125388264656\n",
      "Loss and prediction by the model after 9750000 steps of gradient descent are 0.20711655914783478 0.988200843334198\n",
      "Loss and prediction by the model after 9760000 steps of gradient descent are 0.2097574770450592 0.2750527858734131\n",
      "Loss and prediction by the model after 9770000 steps of gradient descent are 0.20681710541248322 0.6280230283737183\n",
      "Loss and prediction by the model after 9780000 steps of gradient descent are 0.2065441608428955 0.5880038142204285\n",
      "Loss and prediction by the model after 9790000 steps of gradient descent are 0.20574910938739777 0.9793773889541626\n",
      "Loss and prediction by the model after 9800000 steps of gradient descent are 0.20541636645793915 0.8834425210952759\n",
      "Loss and prediction by the model after 9810000 steps of gradient descent are 0.20496150851249695 0.9997941255569458\n",
      "Loss and prediction by the model after 9820000 steps of gradient descent are 0.20484957098960876 0.6603366136550903\n",
      "Loss and prediction by the model after 9830000 steps of gradient descent are 0.20411480963230133 0.9723672866821289\n",
      "Loss and prediction by the model after 9840000 steps of gradient descent are 0.2037598192691803 0.900180459022522\n",
      "Loss and prediction by the model after 9850000 steps of gradient descent are 0.22577758133411407 0.10140156745910645\n",
      "Loss and prediction by the model after 9860000 steps of gradient descent are 0.29663681983947754 0.046176016330718994\n",
      "Loss and prediction by the model after 9870000 steps of gradient descent are 0.2026042938232422 0.9959768056869507\n",
      "Loss and prediction by the model after 9880000 steps of gradient descent are 0.20223626494407654 0.9984786510467529\n",
      "Loss and prediction by the model after 9890000 steps of gradient descent are 0.2018301635980606 0.9978692531585693\n",
      "Loss and prediction by the model after 9900000 steps of gradient descent are 0.20461733639240265 0.26945164799690247\n",
      "Loss and prediction by the model after 9910000 steps of gradient descent are 0.20129284262657166 0.7487995624542236\n",
      "Loss and prediction by the model after 9920000 steps of gradient descent are 0.20126882195472717 0.583712637424469\n",
      "Loss and prediction by the model after 9930000 steps of gradient descent are 0.2004648745059967 0.9813822507858276\n",
      "Loss and prediction by the model after 9940000 steps of gradient descent are 0.20019499957561493 0.9014071822166443\n",
      "Loss and prediction by the model after 9950000 steps of gradient descent are 0.20003806054592133 0.6767504811286926\n",
      "Loss and prediction by the model after 9960000 steps of gradient descent are 0.19933170080184937 0.9699211716651917\n",
      "Loss and prediction by the model after 9970000 steps of gradient descent are 0.1990087628364563 0.9921409487724304\n",
      "Loss and prediction by the model after 9980000 steps of gradient descent are 0.19862660765647888 0.973491907119751\n",
      "Loss and prediction by the model after 9990000 steps of gradient descent are 0.19840791821479797 0.8052877187728882\n",
      "Loss and prediction by the model after 10000000 steps of gradient descent are 0.1979241818189621 0.983834981918335\n",
      "Loss and prediction by the model after 10010000 steps of gradient descent are 0.1978389471769333 0.6708694100379944\n",
      "Loss and prediction by the model after 10020000 steps of gradient descent are 0.19731537997722626 0.9274941682815552\n",
      "Loss and prediction by the model after 10030000 steps of gradient descent are 0.1969534456729889 0.924889087677002\n",
      "Loss and prediction by the model after 10040000 steps of gradient descent are 0.19658368825912476 0.9427745938301086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss and prediction by the model after 10050000 steps of gradient descent are 0.1962539702653885 0.9623996019363403\n",
      "Loss and prediction by the model after 10060000 steps of gradient descent are 0.1959318369626999 0.995158851146698\n",
      "Loss and prediction by the model after 10070000 steps of gradient descent are 0.19562630355358124 0.878131628036499\n",
      "Loss and prediction by the model after 10080000 steps of gradient descent are 0.1952362358570099 0.9748936295509338\n",
      "Loss and prediction by the model after 10090000 steps of gradient descent are 0.19502846896648407 0.8288817405700684\n",
      "Loss and prediction by the model after 10100000 steps of gradient descent are 0.19459249079227448 0.9998683929443359\n",
      "Loss and prediction by the model after 10110000 steps of gradient descent are 0.19422607123851776 0.9600477814674377\n",
      "Loss and prediction by the model after 10120000 steps of gradient descent are 0.19417092204093933 0.6676244735717773\n",
      "Loss and prediction by the model after 10130000 steps of gradient descent are 0.19355550408363342 0.9671259522438049\n",
      "Loss and prediction by the model after 10140000 steps of gradient descent are 0.19316647946834564 0.9986215829849243\n",
      "Loss and prediction by the model after 10150000 steps of gradient descent are 0.1934177130460739 0.5576667189598083\n",
      "Loss and prediction by the model after 10160000 steps of gradient descent are 0.19245164096355438 0.9978501796722412\n",
      "Loss and prediction by the model after 10170000 steps of gradient descent are 0.1957334280014038 0.2569613754749298\n",
      "Loss and prediction by the model after 10180000 steps of gradient descent are 0.19178466498851776 0.9201520085334778\n",
      "Loss and prediction by the model after 10190000 steps of gradient descent are 0.20614531636238098 0.12938085198402405\n",
      "Loss and prediction by the model after 10200000 steps of gradient descent are 0.1911279708147049 0.8895135521888733\n",
      "Loss and prediction by the model after 10210000 steps of gradient descent are 0.19068627059459686 0.9968991279602051\n",
      "Loss and prediction by the model after 10220000 steps of gradient descent are 0.190326988697052 0.9883708953857422\n",
      "Loss and prediction by the model after 10230000 steps of gradient descent are 0.19121800363063812 0.4381658136844635\n",
      "Loss and prediction by the model after 10240000 steps of gradient descent are 0.18979094922542572 0.9497289061546326\n",
      "Loss and prediction by the model after 10250000 steps of gradient descent are 0.18954917788505554 0.8820114135742188\n",
      "Loss and prediction by the model after 10260000 steps of gradient descent are 0.18976294994354248 0.5557544827461243\n",
      "Loss and prediction by the model after 10270000 steps of gradient descent are 0.1892848163843155 0.6402153372764587\n",
      "Loss and prediction by the model after 10280000 steps of gradient descent are 0.18858487904071808 0.9875338077545166\n",
      "Loss and prediction by the model after 10290000 steps of gradient descent are 0.1882263869047165 0.9990352392196655\n",
      "Loss and prediction by the model after 10300000 steps of gradient descent are 0.18790727853775024 0.9821500778198242\n",
      "Loss and prediction by the model after 10310000 steps of gradient descent are 0.18971483409404755 0.3284164071083069\n",
      "Loss and prediction by the model after 10320000 steps of gradient descent are 0.1873866319656372 0.791033148765564\n",
      "Loss and prediction by the model after 10330000 steps of gradient descent are 0.18688304722309113 0.9412004351615906\n",
      "Loss and prediction by the model after 10340000 steps of gradient descent are 1.9273585081100464 0.005212859716266394\n",
      "Loss and prediction by the model after 10350000 steps of gradient descent are 0.18646058440208435 0.8461126089096069\n",
      "Loss and prediction by the model after 10360000 steps of gradient descent are 0.18635398149490356 0.6468162536621094\n",
      "Loss and prediction by the model after 10370000 steps of gradient descent are 0.18568824231624603 0.9757947325706482\n",
      "Loss and prediction by the model after 10380000 steps of gradient descent are 0.1853574961423874 0.9369489550590515\n",
      "Loss and prediction by the model after 10390000 steps of gradient descent are 0.23411434888839722 0.06958555430173874\n",
      "Loss and prediction by the model after 10400000 steps of gradient descent are 0.18509145081043243 0.7250428795814514\n",
      "Loss and prediction by the model after 10410000 steps of gradient descent are 0.18702803552150726 0.3135896921157837\n",
      "Loss and prediction by the model after 10420000 steps of gradient descent are 0.1842460334300995 0.9908393025398254\n",
      "Loss and prediction by the model after 10430000 steps of gradient descent are 0.18623165786266327 0.3250565528869629\n",
      "Loss and prediction by the model after 10440000 steps of gradient descent are 0.18385228514671326 0.8551954627037048\n",
      "Loss and prediction by the model after 10450000 steps of gradient descent are 0.1836937963962555 0.7156655192375183\n",
      "Loss and prediction by the model after 10460000 steps of gradient descent are 0.1834174245595932 0.68798828125\n",
      "Loss and prediction by the model after 10470000 steps of gradient descent are 0.18276140093803406 0.9936625957489014\n",
      "Loss and prediction by the model after 10480000 steps of gradient descent are 0.1824350208044052 0.9948201179504395\n",
      "Loss and prediction by the model after 10490000 steps of gradient descent are 0.1874978393316269 0.22078849375247955\n",
      "Loss and prediction by the model after 10500000 steps of gradient descent are 0.18226952850818634 0.6690868139266968\n",
      "Loss and prediction by the model after 10510000 steps of gradient descent are 0.18254289031028748 0.4835210144519806\n",
      "Loss and prediction by the model after 10520000 steps of gradient descent are 0.18133914470672607 0.9361919164657593\n",
      "Loss and prediction by the model after 10530000 steps of gradient descent are 0.1813976913690567 0.6429405212402344\n",
      "Loss and prediction by the model after 10540000 steps of gradient descent are 0.18096064031124115 0.7202128171920776\n",
      "Loss and prediction by the model after 10550000 steps of gradient descent are 0.18047946691513062 0.9927334785461426\n",
      "Loss and prediction by the model after 10560000 steps of gradient descent are 0.1803007423877716 0.8576363325119019\n",
      "Loss and prediction by the model after 10570000 steps of gradient descent are 0.1799078732728958 0.9997462630271912\n",
      "Loss and prediction by the model after 10580000 steps of gradient descent are 0.18001367151737213 0.6581931114196777\n",
      "Loss and prediction by the model after 10590000 steps of gradient descent are 0.17943526804447174 0.9079927802085876\n",
      "Loss and prediction by the model after 10600000 steps of gradient descent are 0.17931002378463745 0.8408190608024597\n",
      "Loss and prediction by the model after 10610000 steps of gradient descent are 0.17936921119689941 0.5796996355056763\n",
      "Loss and prediction by the model after 10620000 steps of gradient descent are 0.1786975860595703 0.8777168989181519\n",
      "Loss and prediction by the model after 10630000 steps of gradient descent are 0.17843571305274963 0.907433271408081\n",
      "Loss and prediction by the model after 10640000 steps of gradient descent are 0.1785580962896347 0.6051663756370544\n",
      "Loss and prediction by the model after 10650000 steps of gradient descent are 0.1777726709842682 0.9718730449676514\n",
      "Loss and prediction by the model after 10660000 steps of gradient descent are 0.1775844544172287 0.9019113779067993\n",
      "Loss and prediction by the model after 10670000 steps of gradient descent are 0.17728976905345917 0.9671305418014526\n",
      "Loss and prediction by the model after 10680000 steps of gradient descent are 0.1773461252450943 0.7466369271278381\n",
      "Loss and prediction by the model after 10690000 steps of gradient descent are 1.7955743074417114 0.005596357397735119\n",
      "Loss and prediction by the model after 10700000 steps of gradient descent are 0.17771320044994354 0.43704405426979065\n",
      "Loss and prediction by the model after 10710000 steps of gradient descent are 0.17640182375907898 0.7726367712020874\n",
      "Loss and prediction by the model after 10720000 steps of gradient descent are 0.17645426094532013 0.5866930484771729\n",
      "Loss and prediction by the model after 10730000 steps of gradient descent are 0.1756483018398285 0.9783505797386169\n",
      "Loss and prediction by the model after 10740000 steps of gradient descent are 0.17625613510608673 0.5187464356422424\n",
      "Loss and prediction by the model after 10750000 steps of gradient descent are 0.1752178966999054 0.9997701048851013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss and prediction by the model after 10760000 steps of gradient descent are 0.174916610121727 0.9549530148506165\n",
      "Loss and prediction by the model after 10770000 steps of gradient descent are 0.17548562586307526 0.4962983727455139\n",
      "Loss and prediction by the model after 10780000 steps of gradient descent are 0.17440497875213623 0.8611136674880981\n",
      "Loss and prediction by the model after 10790000 steps of gradient descent are 0.7669710516929626 0.013386309146881104\n",
      "Loss and prediction by the model after 10800000 steps of gradient descent are 0.17389635741710663 0.9995191097259521\n",
      "Loss and prediction by the model after 10810000 steps of gradient descent are 0.1736632138490677 0.8437511920928955\n",
      "Loss and prediction by the model after 10820000 steps of gradient descent are 0.17321372032165527 0.9999107122421265\n",
      "Loss and prediction by the model after 10830000 steps of gradient descent are 0.18910931050777435 0.1298094391822815\n",
      "Loss and prediction by the model after 10840000 steps of gradient descent are 0.17283497750759125 0.807786762714386\n",
      "Loss and prediction by the model after 10850000 steps of gradient descent are 0.17273174226284027 0.809668242931366\n",
      "Loss and prediction by the model after 10860000 steps of gradient descent are 0.17237834632396698 0.9940018057823181\n",
      "Loss and prediction by the model after 10870000 steps of gradient descent are 0.17213720083236694 0.9865835309028625\n",
      "Loss and prediction by the model after 10880000 steps of gradient descent are 0.1728077530860901 0.4800962507724762\n",
      "Loss and prediction by the model after 10890000 steps of gradient descent are 0.17156760394573212 0.9658440351486206\n",
      "Loss and prediction by the model after 10900000 steps of gradient descent are 0.1713883876800537 0.8598912954330444\n",
      "Loss and prediction by the model after 10910000 steps of gradient descent are 0.1754361242055893 0.25044140219688416\n",
      "Loss and prediction by the model after 10920000 steps of gradient descent are 0.1714145988225937 0.5695908665657043\n",
      "Loss and prediction by the model after 10930000 steps of gradient descent are 0.17052915692329407 0.9840191006660461\n",
      "Loss and prediction by the model after 10940000 steps of gradient descent are 0.17025259137153625 0.9937877058982849\n",
      "Loss and prediction by the model after 10950000 steps of gradient descent are 0.17100130021572113 0.47957900166511536\n",
      "Loss and prediction by the model after 10960000 steps of gradient descent are 0.17145304381847382 0.38541316986083984\n",
      "Loss and prediction by the model after 10970000 steps of gradient descent are 0.1703089475631714 0.5223992466926575\n",
      "Loss and prediction by the model after 10980000 steps of gradient descent are 0.17043446004390717 0.44224777817726135\n",
      "Loss and prediction by the model after 10990000 steps of gradient descent are 0.16925249993801117 0.7190027236938477\n",
      "Loss and prediction by the model after 11000000 steps of gradient descent are 0.16870354115962982 0.9998264312744141\n",
      "Loss and prediction by the model after 11010000 steps of gradient descent are 0.1690950095653534 0.5530133843421936\n",
      "Loss and prediction by the model after 11020000 steps of gradient descent are 0.1690634787082672 0.4845728278160095\n",
      "Loss and prediction by the model after 11030000 steps of gradient descent are 0.1691785603761673 0.4216817319393158\n",
      "Loss and prediction by the model after 11040000 steps of gradient descent are 0.16771292686462402 0.8284716606140137\n",
      "Loss and prediction by the model after 11050000 steps of gradient descent are 0.16730575263500214 0.9976447820663452\n",
      "Loss and prediction by the model after 11060000 steps of gradient descent are 0.1680636703968048 0.5136610865592957\n",
      "Loss and prediction by the model after 11070000 steps of gradient descent are 0.16712290048599243 0.7875944972038269\n",
      "Loss and prediction by the model after 11080000 steps of gradient descent are 0.16727466881275177 0.5728524327278137\n",
      "Loss and prediction by the model after 11090000 steps of gradient descent are 0.16653116047382355 0.8527877330780029\n",
      "Loss and prediction by the model after 11100000 steps of gradient descent are 0.16778826713562012 0.41098371148109436\n",
      "Loss and prediction by the model after 11110000 steps of gradient descent are 0.22999779880046844 0.06274216622114182\n",
      "Loss and prediction by the model after 11120000 steps of gradient descent are 0.16592219471931458 0.9984500408172607\n",
      "Loss and prediction by the model after 11130000 steps of gradient descent are 0.16574737429618835 0.9052364230155945\n",
      "Loss and prediction by the model after 11140000 steps of gradient descent are 0.16611136496067047 0.5502299666404724\n",
      "Loss and prediction by the model after 11150000 steps of gradient descent are 0.16516946256160736 0.9996756315231323\n",
      "Loss and prediction by the model after 11160000 steps of gradient descent are 0.16502541303634644 0.959982693195343\n",
      "Loss and prediction by the model after 11170000 steps of gradient descent are 0.1649191975593567 0.8170386552810669\n",
      "Loss and prediction by the model after 11180000 steps of gradient descent are 0.16454951465129852 0.9996048212051392\n",
      "Loss and prediction by the model after 11190000 steps of gradient descent are 0.16422998905181885 0.9954721331596375\n",
      "Loss and prediction by the model after 11200000 steps of gradient descent are 0.16400207579135895 0.9918375611305237\n",
      "Loss and prediction by the model after 11210000 steps of gradient descent are 0.16399122774600983 0.7506093382835388\n",
      "Loss and prediction by the model after 11220000 steps of gradient descent are 0.16347579658031464 0.971365749835968\n",
      "Loss and prediction by the model after 11230000 steps of gradient descent are 0.16336922347545624 0.8011977076530457\n",
      "Loss and prediction by the model after 11240000 steps of gradient descent are 0.1631658673286438 0.7938046455383301\n",
      "Loss and prediction by the model after 11250000 steps of gradient descent are 0.1628454327583313 0.9224425554275513\n",
      "Loss and prediction by the model after 11260000 steps of gradient descent are 0.1627964824438095 0.7368944883346558\n",
      "Loss and prediction by the model after 11270000 steps of gradient descent are 0.16234226524829865 0.9768738150596619\n",
      "Loss and prediction by the model after 11280000 steps of gradient descent are 0.16209959983825684 0.9700131416320801\n",
      "Loss and prediction by the model after 11290000 steps of gradient descent are 0.16200493276119232 0.7707726955413818\n",
      "Loss and prediction by the model after 11300000 steps of gradient descent are 0.16189217567443848 0.6850613951683044\n",
      "Loss and prediction by the model after 11310000 steps of gradient descent are 0.16166655719280243 0.6755756735801697\n",
      "Loss and prediction by the model after 11320000 steps of gradient descent are 0.16123844683170319 0.9960333704948425\n",
      "Loss and prediction by the model after 11330000 steps of gradient descent are 0.16096796095371246 0.9858595728874207\n",
      "Loss and prediction by the model after 11340000 steps of gradient descent are 0.16085827350616455 0.7839677929878235\n",
      "Loss and prediction by the model after 11350000 steps of gradient descent are 0.1606987565755844 0.8668001890182495\n",
      "Loss and prediction by the model after 11360000 steps of gradient descent are 0.16069334745407104 0.6905036568641663\n",
      "Loss and prediction by the model after 11370000 steps of gradient descent are 0.1612752377986908 0.4563191533088684\n",
      "Loss and prediction by the model after 11380000 steps of gradient descent are 0.16021357476711273 0.6908652782440186\n",
      "Loss and prediction by the model after 11390000 steps of gradient descent are 0.15974195301532745 0.8159687519073486\n",
      "Loss and prediction by the model after 11400000 steps of gradient descent are 0.16119666397571564 0.3909473717212677\n",
      "Loss and prediction by the model after 11410000 steps of gradient descent are 0.1598602682352066 0.6053157448768616\n",
      "Loss and prediction by the model after 11420000 steps of gradient descent are 0.16177786886692047 0.3239360451698303\n",
      "Loss and prediction by the model after 11430000 steps of gradient descent are 0.15884838998317719 0.9997401833534241\n",
      "Loss and prediction by the model after 11440000 steps of gradient descent are 0.15867741405963898 0.9920639991760254\n",
      "Loss and prediction by the model after 11450000 steps of gradient descent are 0.1584383100271225 0.9684731364250183\n",
      "Loss and prediction by the model after 11460000 steps of gradient descent are 0.15865883231163025 0.8097743391990662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss and prediction by the model after 11470000 steps of gradient descent are 0.15850937366485596 0.7260699272155762\n",
      "Loss and prediction by the model after 11480000 steps of gradient descent are 0.15809383988380432 0.999543309211731\n",
      "Loss and prediction by the model after 11490000 steps of gradient descent are 0.1711796224117279 0.1494193971157074\n",
      "Loss and prediction by the model after 11500000 steps of gradient descent are 0.15825548768043518 0.5914369225502014\n",
      "Loss and prediction by the model after 11510000 steps of gradient descent are 0.1577170193195343 0.8087450265884399\n",
      "Loss and prediction by the model after 11520000 steps of gradient descent are 0.15734080970287323 0.9758214950561523\n",
      "Loss and prediction by the model after 11530000 steps of gradient descent are 0.15709394216537476 0.9728158712387085\n",
      "Loss and prediction by the model after 11540000 steps of gradient descent are 0.15777099132537842 0.5231310129165649\n",
      "Loss and prediction by the model after 11550000 steps of gradient descent are 0.156744122505188 0.9246807098388672\n",
      "Loss and prediction by the model after 11560000 steps of gradient descent are 0.15652748942375183 0.9621217846870422\n",
      "Loss and prediction by the model after 11570000 steps of gradient descent are 0.15650279819965363 0.7710222601890564\n",
      "Loss and prediction by the model after 11580000 steps of gradient descent are 0.15611743927001953 0.9459787607192993\n",
      "Loss and prediction by the model after 11590000 steps of gradient descent are 0.15588201582431793 0.9695857763290405\n",
      "Loss and prediction by the model after 11600000 steps of gradient descent are 0.15601468086242676 0.7068278789520264\n",
      "Loss and prediction by the model after 11610000 steps of gradient descent are 0.15553025901317596 0.9903407096862793\n",
      "Loss and prediction by the model after 11620000 steps of gradient descent are 0.15532377362251282 0.9380354285240173\n",
      "Loss and prediction by the model after 11630000 steps of gradient descent are 0.1550629436969757 0.9414505958557129\n",
      "Loss and prediction by the model after 11640000 steps of gradient descent are 0.15739256143569946 0.3313765823841095\n",
      "Loss and prediction by the model after 11650000 steps of gradient descent are 0.15464267134666443 0.9407744407653809\n",
      "Loss and prediction by the model after 11660000 steps of gradient descent are 0.15445293486118317 0.9954436421394348\n",
      "Loss and prediction by the model after 11670000 steps of gradient descent are 0.19844792783260345 0.07978108525276184\n",
      "Loss and prediction by the model after 11680000 steps of gradient descent are 0.15407694876194 0.9122006297111511\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-665fdd42e868>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m#find gradients of totalt wrt yarray.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtotalloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mgradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate=0.00005\n",
    "num_descents=40000000 #number of times gradient descent is employed\n",
    "\n",
    "img=img.reshape(784).cuda()\n",
    "\n",
    "for i in range(num_descents):\n",
    "    \n",
    "    imagef=img+diff\n",
    "    pred=torch.exp(model(imagef.reshape([-1,1,28,28])))[0][8]\n",
    "    totalloss=findloss(diff,pred)\n",
    "    if i%10000 ==0:\n",
    "        print('Loss and prediction by the model after '+str(i)+' steps of gradient descent are '+str(totalloss.item()),str(pred.item()))\n",
    "\n",
    "    #find gradients of totalt wrt yarray.\n",
    "    totalloss.backward()\n",
    "\n",
    "    gradients=diff.grad\n",
    "    with torch.no_grad():\n",
    "        diff[1:]=diff[1:]-learning_rate*gradients[1:]\n",
    "        \n",
    "        \n",
    "    diff.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d5042aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6aa97407f0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(imagef.detach().cpu().numpy().reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "893b1a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6aa970ee50>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(diff.detach().cpu().numpy().reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5828e718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-121c0ab709c4>:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss and prediction by the model after 1080000 steps of gradient descent are 0.5179365873336792 0.5206899046897888\n",
      "Loss and prediction by the model after 1090000 steps of gradient descent are 0.5179376602172852 0.6639869213104248\n",
      "Loss and prediction by the model after 1100000 steps of gradient descent are 0.5179382562637329 0.9865331649780273\n",
      "Loss and prediction by the model after 1110000 steps of gradient descent are 0.5179411172866821 0.6558887362480164\n",
      "Loss and prediction by the model after 1120000 steps of gradient descent are 0.5179430842399597 0.7715947031974792\n",
      "Loss and prediction by the model after 1130000 steps of gradient descent are 0.5179464817047119 0.9635094404220581\n",
      "Loss and prediction by the model after 1140000 steps of gradient descent are 0.5179803967475891 0.18859560787677765\n",
      "Loss and prediction by the model after 1150000 steps of gradient descent are 0.5179645419120789 0.9873834848403931\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-b0c093d8768c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m#find gradients of totalt wrt yarray.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtotalloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mgradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate=0.00001\n",
    "num_descents=6000000 #number of times gradient descent is employed\n",
    "\n",
    "img=img.reshape(784).cuda()\n",
    "\n",
    "for i in range(1080000,num_descents):\n",
    "    \n",
    "    imagef=img+diff\n",
    "    pred=torch.exp(model(imagef.reshape([-1,1,28,28])))[0][8]\n",
    "    totalloss=findloss(diff,pred)\n",
    "    if i%10000 ==0:\n",
    "        print('Loss and prediction by the model after '+str(i)+' steps of gradient descent are '+str(totalloss.item()),str(pred.item()))\n",
    "\n",
    "    #find gradients of totalt wrt yarray.\n",
    "    totalloss.backward()\n",
    "\n",
    "    gradients=diff.grad\n",
    "    with torch.no_grad():\n",
    "        diff[1:]=diff[1:]-learning_rate*gradients[1:]\n",
    "        \n",
    "        \n",
    "    diff.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "49726ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6aa980a850>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(imagef.detach().cpu().numpy().reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "da493838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6aa97dafa0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(diff.detach().cpu().numpy().reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44e0f41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
